<!DOCTYPE html>
<html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Rohan &amp; Lenny #1: Neural Networks &amp; The Backpropagation Algorithm, Explained</title><link rel="canonical" href="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d"><meta name="title" content="Rohan &amp; Lenny #1: Neural Networks &amp; The Backpropagation Algorithm, Explained"><meta name="referrer" content="always"><meta name="description" content="Do you know the chain rule? Then you know the neural network backpropagation algorithm!"><meta property="og:title" content="Rohan &amp; Lenny #1: Neural Networks &amp; The Backpropagation Algorithm, Explained"><meta property="og:url" content="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*pKv3DL-enonlNJxzqhlWkQ.jpeg"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="Do you know the chain rule? Then you know the neural network backpropagation algorithm!"><meta name="twitter:description" content="Do you know the chain rule? Then you know the neural network backpropagation algorithm!"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*pKv3DL-enonlNJxzqhlWkQ.jpeg"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://ayearofai.com/@mckapur"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="https://facebook.com/1004843339565980"><meta property="fb:smart_publish:robots" content="noauto"><meta property="article:published_time" content="2016-03-04T04:43:20.635Z"><meta name="twitter:creator" content="@MCKapur"><meta name="twitter:site" content="@mckapur"><meta property="og:site_name" content="A year of Artificial Intelligence"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="30 min read"><script src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/ga.js" async="" type="text/javascript"></script><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":1279,"url":"https://cdn-images-1.medium.com/max/1920/1*pKv3DL-enonlNJxzqhlWkQ.jpeg"},"datePublished":"2016-03-04T04:43:20.635Z","dateModified":"2016-05-22T03:40:25.551Z","headline":"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","name":"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","author":{"@type":"Person","name":"Rohan Kapur","url":"https://medium.com/@mckapur"},"publisher":{"@type":"Organization","name":"A year of Artificial Intelligence","url":"https://ayearofai.com","logo":{"@type":"ImageObject","width":90,"height":60,"url":"https://cdn-images-1.medium.com/max/90/1*NZsNSuNxe_O2YW1ybboOvA.jpeg"}}}</script><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/abf4609d4f9d"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/abf4609d4f9d"><meta property="al:android:url" content="medium://p/abf4609d4f9d"><meta property="al:web:url" content="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/abf4609d4f9d"><meta name="theme-color" content="#000000"><meta name="robots" content="index, follow"><link rel="stylesheet" type="text/css" href="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/fonts-base.css"><link rel="stylesheet" href="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/main-base.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}var _gaq = _gaq || [];_gaq.push(["_setAccount", "UA-24232453-2"]); _gaq.push(["_setDomainName", window.location.hostname]); _gaq.push(["_setAllowLinker", true]); _gaq.push(["_trackPageview"]);_asyncScript(("https:" == document.location.protocol ? "https://ssl" : "http://www") + ".google-analytics.com/ga.js");(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="shortcut icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico" class="js-favicon is-default"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg" color="#171717"></head><body data-action-scope="_actionscope_0" itemscope="" class=" postShowScreen browser-firefox is-withMagicUnderlines is-js"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div data-action-scope="_actionscope_1" class="butterBar butterBar--error"></div><div style="display: block; visibility: visible;" id="_obv.shell._surface_1474420771267" class="surface"><div data-action-scope="_actionscope_2" data-used="true" class="screenContent surface-content is-supplementalPostContentLoaded"><canvas height="673" width="1291" class="canvas-renderer"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"><div data-action-scope="_actionscope_4" class="notesMarkers"></div></div><div class="metabar u-clearfix js-metabar metabar--light metabar--tall metabar--hiddenWhenMinimized u-tintBgColor u-tintSpectrum metabar--affixed is-minimized"><div class="metabar-inner u-marginAuto u-maxWidth1000 u-paddingLeft20 u-paddingRight20 js-metabarMiddle is-hideWhenMinimized"><div class="metabar-block metabar-block--left u-floatLeft u-height65 u-xs-height56"><span class="u-inlineBlock u-paddingTop12 u-xs-show"><a class="link avatar avatar--roundedRectangle u-baseColor--link" href="https://ayearofai.com/?source=avatar-lo_91b04744946-bb87da25612c" title="Go to A year of Artificial Intelligence" aria-label="Go to A year of Artificial Intelligence" data-action-source="avatar-lo_91b04744946-bb87da25612c" data-collection-slug="a-year-of-artificial-intelligence"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1NZsNSuNxe_O2YW1ybboOvA_003.jpeg" class="avatar-image avatar-image--icon" alt="A year of Artificial Intelligence"></a></span></div><div class="metabar-block metabar-block--center u-height65 u-xs-height56"><a class="metabar-logoWrapper js-logCollection u-paddingRight10 u-xs-hide" href="https://ayearofai.com/?source=logo-lo_91b04744946-bb87da25612c"><img class="metabar-logo" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1NZsNSuNxe_O2YW1ybboOvA_002.jpeg" alt="A year of Artificial Intelligence"></a></div><div class="metabar-block metabar-block--right u-floatRight u-height65 u-xs-height56"><div class="buttonSet u-marginRight25"><button class="button button--chromeless u-noUserSelect u-baseColor--buttonNormal js-relationshipButton is-chromeless" data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence/abf4609d4f9d" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="buttonSet"><a class="button button--primary button--light button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-marginRight15 u-lineHeight30 u-height32" href="https://medium.com/m/signin?redirect=https%3A%2F%2Fayearofai.com%2Frohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" data-action="sign-in-prompt" data-redirect="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d" data-action-source="nav_signup">Sign in / Sign up</a><label class="inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--placeholderNormal u-lineHeight30 u-height32 u-verticalAlignMiddle" title="Search Medium"><span class="svgIcon svgIcon--search svgIcon--25px u-fillTransparentBlackNormal u-baseColor--iconNormal"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent u-lineHeight30 u-height32 u-verticalAlignMiddle" placeholder="Search Medium" required="true" type="search"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon" href="https://medium.com/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a></div></div></div><div class="metabar-inner u-marginAuto u-maxWidth1000 js-metabarBottom"><div class="metabar-block metabar-block--below u-height50 u-xs-height39"><div class="u-fadeRight"><div class="u-textAlignCenter u-xs-textAlignLeft u-overflowX js-collectionNavItems u-paddingLeft20 u-paddingRight20"><span class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSizeSmaller u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal"><a class="link link--darken u-accentColor--textDarken u-baseColor--link" href="https://ayearofai.com/tagged/today-i-learned">Today I Learned</a></span><span class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSizeSmaller u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal"><a class="link link--darker link--darken u-accentColor--textDarken u-baseColor--link" href="https://ayearofai.com/tagged/algorithms">Algorithms</a></span><span class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSizeSmaller u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal"><a class="link link--darken u-accentColor--textDarken u-baseColor--link" href="https://ayearofai.com/tagged/case-studies">Case Studies</a></span><span class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSizeSmaller u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal"><a class="link link--darken u-accentColor--textDarken u-baseColor--link" href="https://ayearofai.com/tagged/philosophical">Philosophical</a></span><span class="metabar-navItem js-collectionNavItem u-uiTextMedium u-fontSizeSmaller u-inlineBlock u-textUppercase u-letterSpacing003 u-textColorNormal"><a class="link link--darken u-accentColor--textDarken u-baseColor--link" href="https://ayearofai.com/tagged/meta">Meta</a></span></div></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-tintBgColor u-height115 u-xs-height95"></div><div class="postActionsBar js-postActionsBar is-visible"><div class="postActionsBar-container container"><div class="postActionsBar-content row js-postActionsBarContent"><div class="postActions u-marginAuto u-maxWidth1250 u-padding0 u-positionRelative"><div class="u-positionAbsolute u-positionRight0 u-marginRight20 js-readNextInteractions"><div class="u-floatLeft"><div class="u-floatRight buttonSet"><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button></div><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button></div></div></div><div class="col u-xs-hide u-height44 u-width255 u-padding0 u-marginLeft0 u-verticalAlignTop u-foreground u-block u-floatLeft js-readNextMetabarRight"><div class="streamItem streamItem--placementCardMetabar js-streamItem"><div class="readNextPromo u-height44 readNextPromo--chromeless"><div class="buttonSet-separator buttonSet-separator--readNext u-floatLeft u-verticalAlignTop js-postActionRightSeparator"></div><div class="js-trackedPost " data-post-id="ae804fa86cca" data-source="read_next_metabar----------45" data-tracking-context="placement" data-scroll="fixed"><a class="link link--noUnderline u-block u-baseColor--link" href="https://ayearofai.com/rohan-2-time-progress-ae804fa86cca?source=read_next_metabar----------45"><div class="readNextPromo-postInfo"><span class="readNextPromo-callToAction u-accentColor--textNormal u-uiTextBold">Next story</span><h4 class="readNextPromo-postTitle u-block u-noWrapWithEllipsis u-contentSansBold ">Rohan #2: Artificial Intelligence, a summary — ∂Progress/∂Time</h4></div></a></div></div></div></div></div><div class="container u-maxWidth740 u-height44 u-marginAuto js-postActions-actionButtons"><div class="u-floatLeft buttonSet buttonSet--withLabels"><div class="buttonSet-inner"><div class="js-actionRecommend" data-post-id="abf4609d4f9d" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_actions_bar"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton" title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/abf4609d4f9d" data-action-source="post_actions_bar"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="show-recommends" data-action-value="abf4609d4f9d">34</button></div></div><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="scroll-to-responses" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="scroll-to-responses">1</button></div></div></div></div></div></div></div><article class=" u-sizeViewHeightMin100 postArticle postArticle--full is-withAccentColors" lang="en"><header class="container u-maxWidth740"><div class="postMetaHeader u-paddingBottom10 row"><div class="col u-size12of12 js-postMetaLockup"><div class="postMetaLockup postMetaLockup--authorWithBio u-flex js-postMetaLockup"><div class="u-flex0"><a class="link avatar u-baseColor--link" href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1f_drXOgflIo8yaxCbSDOmQ_003.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><a class="link link link--darken link--darker u-baseColor--link" href="https://ayearofai.com/@mckapur?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button u-paddingLeft10 u-paddingRight10 u-height18 u-lineHeight16 u-verticalAlignMiddle u-fontSizeSmallest u-uiTextMedium u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-marginTopNegative2 u-xs-hide" data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary u-paddingLeft10 u-paddingRight10 u-height18 u-lineHeight16 u-verticalAlignMiddle u-fontSizeSmallest u-uiTextMedium u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-marginTopNegative2 u-xs-hide" data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="post_header_lockup_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span><div class="postMetaInline u-noWrapWithEllipsis u-xs-normalWrap u-xs-lineClamp2">rohankapur.com</div><div class="postMetaInline js-testPostMetaInlineSupplemental">Mar 3<span class="middotDivider"></span><span class="readingTime">30 min read</span></div></div></div></div></div></header><main data-scroll="native" role="main" class="postArticle-content js-postField js-notesSource js-trackedPost" data-post-id="abf4609d4f9d" data-source="post_page" data-collection-id="bb87da25612c" data-tracking-context="postPage"><section name="5f42" class="section section--body section--first"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--fullWidth"><figure name="4d27" id="4d27" class="graf graf--figure graf--layoutFillWidth graf--leading"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.7%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*pKv3DL-enonlNJxzqhlWkQ.jpeg" data-width="2750" data-height="1833" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1pKv3DL-enonlNJxzqhlWkQ_002.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="47" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1pKv3DL-enonlNJxzqhlWkQ.jpeg" class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*pKv3DL-enonlNJxzqhlWkQ.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*pKv3DL-enonlNJxzqhlWkQ.jpeg"></noscript></div></div></figure></div><div class="section-inner layoutSingleColumn"><h1 name="ae68" id="ae68" class="graf graf--h3 graf-after--figure graf--title">Rohan &amp; Lenny #1: Neural Networks &amp; The Backpropagation Algorithm, Explained</h1><h2 name="892d" id="892d" class="graf graf--h4 graf-after--h3 graf--last graf--subtitle">Do you know the chain rule? Then you know the neural network backpropagation algorithm!</h2></div></div></section><section name="bf29" class="section section--body"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner layoutSingleColumn"><blockquote name="e8ab" id="e8ab" class="graf graf--pullquote graf--leading graf--last">This is the first group (<a href="https://medium.com/u/de8e2540b759" data-href="https://medium.com/u/de8e2540b759" data-anchor-type="2" data-user-id="de8e2540b759" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user">Lenny</a> and <a href="https://medium.com/u/cb55958ea3bb" data-href="https://medium.com/u/cb55958ea3bb" data-anchor-type="2" data-user-id="cb55958ea3bb" data-action="show-user-card" data-action-type="hover" class="markup--user markup--pullquote-user">Rohan</a>) entry in our <a href="https://medium.com/a-year-of-artificial-intelligence" data-href="https://medium.com/a-year-of-artificial-intelligence" class="markup--anchor markup--pullquote-anchor">journey</a> to extend our knowledge of Artificial Intelligence in the year of 2016. Learn more about our motives in this <a href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" data-href="https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5" class="markup--anchor markup--pullquote-anchor">introduction</a>&nbsp;post.</blockquote></div></div></section><section name="6590" class="section section--body"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner layoutSingleColumn"><p name="1716" id="1716" class="graf graf--p graf--leading">In
 Rohan’s last post, he talked about evaluating and plugging holes in his
 knowledge of machine learning thus far. The backpropagation 
algorithm — the process of training a neural network — was a glaring one
 for both of us in particular. Together, we embarked on mastering 
backprop through some great online lectures from professors at MIT &amp;
 Stanford. After attempting a few programming implementations and hand 
solutions, we felt equipped to write an article for AYOAI — together.</p><p name="d32d" id="d32d" class="graf graf--p graf-after--p">Today,
 we’ll do our best to explain backpropagation and neural networks from 
the beginning. If you have an elementary understanding of differential 
calculus and perhaps an intuition of what machine learning is, we hope 
you come out of this blog post with an (acute, but existent nonetheless)
 understanding of neural networks and how to train them. Let us know if 
we succeeded!</p><h4 name="483f" id="483f" class="graf graf--h4 graf-after--p">Introduction to Neural&nbsp;Networks</h4><p name="bbc0" id="bbc0" class="graf graf--p graf-after--h4">Let’s
 start off with a quick introduction to the concept of neural networks. 
Fundamentally, neural networks are nothing more than really good 
function approximators — you give a trained network an input vector, it 
performs a series of operations, and it produces an output vector. To 
train our network to estimate an unknown function, we give it a 
collection of data points — which we denote the “training set” — that 
the network will learn from and generalize on to make future inferences.</p><figure name="4df3" id="4df3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.099999999999994%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*m0tn6W0ipC3Ro7mo." data-width="768" data-height="362" data-action="zoom" data-action-value="0*m0tn6W0ipC3Ro7mo." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0m0tn6W0ipC3Ro7mo_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="35" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0m0tn6W0ipC3Ro7mo.png" class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*m0tn6W0ipC3Ro7mo."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*m0tn6W0ipC3Ro7mo."></noscript></div></div><figcaption class="imageCaption">This is what a neural network looks like. Each circle is a <strong class="markup--strong markup--figure-strong">neuron</strong>, and the arrows are connections between neurons in consecutive&nbsp;<strong class="markup--strong markup--figure-strong">layers</strong>.</figcaption></figure><p name="44f8" id="44f8" class="graf graf--p graf-after--figure">Neural networks are structured as a series of <strong class="markup--strong markup--p-strong">layers</strong>, each composed of one or more <strong class="markup--strong markup--p-strong">neurons</strong> (as depicted above). Each neuron produces an output, or <strong class="markup--strong markup--p-strong">activation</strong>, based on the outputs of the previous layer and a set of weights.</p><figure name="3850" id="3850" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 406px; max-height: 44px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 10.8%;"></div><img class="graf-image" data-image-id="1*OSspmzZwl9mK_wJgQDcyLA.png" data-width="406" data-height="44" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1OSspmzZwl9mK_wJgQDcyLA.png"></div><figcaption class="imageCaption">This
 is how each neuron computes it’s own activation. It computes a weighted
 sum of the outputs of the previous layer (which, for the inputs, we’ll 
call <strong class="markup--strong markup--figure-strong">x</strong>), 
and applies an activation function (more on that later), before 
forwarding it on to the next layer. Each neuron in a layer has its own 
set of weights — so while each neuron in a layer is looking at the same 
inputs, their outputs will all be different.</figcaption></figure><p name="2003" id="2003" class="graf graf--p graf-after--figure">When
 using a neural network to approximate a function, the data is forwarded
 through the network layer-by-layer until it reaches the final layer. 
The final layer’s activations are the predictions that the network 
actually makes.</p><p name="ff21" id="ff21" class="graf graf--p graf-after--p">All this probably seems kind of magical, but it actually works. The key is finding the right set of <strong class="markup--strong markup--p-strong">weights</strong> for all of the connections to make the right decisions (this happens in a process known as <strong class="markup--strong markup--p-strong">training</strong>) — and that’s what most of this post is going to be about.</p><p name="0c95" id="0c95" class="graf graf--p graf-after--p">When
 we’re training the network, it’s often convenient to have some metric 
of how good or bad we’re doing; we call this metric the cost function. 
Generally speaking, the cost function looks at the function the network 
has inferred and uses it to estimate values for the data points in our 
training set. The discrepancies between the outputs in the estimations 
and the training set data points are the principle values for our cost 
function. When training our network, the goal will be to get the value 
of this cost function as low as possible (we’ll see how to do that in 
just a bit, but for now, just focus on the intuition of what a cost 
function is and what it’s good for). Generally speaking, the cost 
function <em class="markup--em markup--p-em">should</em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em"> </em></strong>be more or less convex, like so:</p><figure name="6921" id="6921" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 533px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.1%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*7i-bEOVyjyVmBOYV." data-width="1130" data-height="860" data-action="zoom" data-action-value="0*7i-bEOVyjyVmBOYV." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/07i-bEOVyjyVmBOYV.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="55" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*7i-bEOVyjyVmBOYV."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*7i-bEOVyjyVmBOYV."></noscript></div></div></figure><p name="c1ed" id="c1ed" class="graf graf--p graf-after--figure">In
 reality, it’s impossible for any network or cost function to be truly 
convex. However, as we’ll soon see, local minima may not be a big deal, 
as long as there is still a general trend for us to follow to get to the
 bottom. Also, notice that the cost function is parameterized by our 
network’s weights — we control our loss function by changing the 
weights.</p><p name="a604" id="a604" class="graf graf--p graf-after--p">One
 last thing to keep in mind about the loss function is that it doesn’t 
just have to capture how correctly your network estimates — it can 
specify any objective that needs to be optimized. For example, you 
generally want to penalize larger weights, as they could lead to 
overfitting. If this is the case, simply adding a <strong class="markup--strong markup--p-strong">regularization term</strong>
 to your cost function that expresses how big your weights will mean 
that, in the process of training your network, it will look for a 
solution that has the best estimates possible while preventing 
overfitting.</p><p name="d312" id="d312" class="graf graf--p graf-after--p">Now,
 let’s take a look at how we can actually minimize the cost function 
during the training process to find a set of weights that work the best 
for our objective.</p><h4 name="8e7d" id="8e7d" class="graf graf--h4 graf-after--p">Minimizing the Cost&nbsp;Function</h4><p name="8a3a" id="8a3a" class="graf graf--p graf-after--h4">Now that we’ve developed a metric for “scoring” our network (which we’ll denote as <strong class="markup--strong markup--p-strong">J(W)</strong>),
 we need to find the weights that will make that score as low as 
possible. If you think back to your pre-calculus days, your first 
instinct might be to set the derivative of the cost function to zero and
 solve, which would give us the locations of every minimum/maximum in 
the function. Unfortunately, there are a few problems with this 
approach:</p><ol class="postList"><li name="4842" id="4842" class="graf graf--li graf-after--p">We
 don’t have a simple equation for our cost function, so computing an 
expression for the derivative and solving it isn’t trivial.</li><li name="d97d" id="d97d" class="graf graf--li graf-after--li">The
 function is many-dimensional (each weight gets its own dimension) — we 
need to find the points where all of those derivatives are zero. Also 
not so trivial.</li><li name="f51e" id="f51e" class="graf graf--li graf-after--li">There
 are lots of minimums and maximums throughout the function, and sorting 
out which one is the one you should be using can be computationally 
expensive.</li></ol><p name="17a6" id="17a6" class="graf graf--p graf-after--li">Especially
 as the size of networks begins to scale up, solving for the weights 
directly becomes increasingly infeasible. Instead, we look at a 
different class of algorithms, called <strong class="markup--strong markup--p-strong">iterative optimization algorithms</strong>, that progressively work their way towards the optimal solution.</p><p name="d021" id="d021" class="graf graf--p graf-after--p">The most basic of these algorithms is <strong class="markup--strong markup--p-strong">gradient descent</strong>. Recall that our cost function will be <em class="markup--em markup--p-em">essentially</em>
 convex, and we want to get as close as possible to the global minimum. 
Instead of solving for it analytically, gradient descent follows the 
derivatives to essentially “roll” down the slope until it finds its way 
to the center.</p><p name="51b3" id="51b3" class="graf graf--p graf-after--p">Let’s take the example of a single-weight neural network, whose cost function is depicted below.</p><figure name="97f2" id="97f2" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 533px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.1%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*M0XaQitZPdQTCl86." data-width="1130" data-height="860" data-action="zoom" data-action-value="0*M0XaQitZPdQTCl86." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0M0XaQitZPdQTCl86.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="55" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*M0XaQitZPdQTCl86."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*M0XaQitZPdQTCl86."></noscript></div></div></figure><p name="62e2" id="62e2" class="graf graf--p graf-after--figure">We
 start off by initializing our weight randomly, which puts us at the red
 dot on the diagram above. Taking the derivative, we see the slope at 
this point is a pretty big positive number. We want to move closer to 
the center — so naturally, we should take a pretty big step in the 
opposite direction of the slope.</p><figure name="3159" id="3159" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 533px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 76.1%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*lF6cKLbGItdwdKl-." data-width="1130" data-height="860" data-action="zoom" data-action-value="0*lF6cKLbGItdwdKl-." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0lF6cKLbGItdwdKl-.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="55" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*lF6cKLbGItdwdKl-."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*lF6cKLbGItdwdKl-."></noscript></div></div></figure><p name="a94a" id="a94a" class="graf graf--p graf-after--figure">If
 we repeat the process enough, we soon find ourselves nearly at the 
bottom of our curve and much closer to the optimal weight configuration 
for our network.</p><p name="4332" id="4332" class="graf graf--p graf-after--p">More formally, gradient descent looks something like this:</p><figure name="297c" id="297c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 255px; max-height: 91px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><img class="graf-image" data-image-id="1*U0VohJIM1F_bE5Msmjzv4Q.png" data-width="255" data-height="91" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1U0VohJIM1F_bE5Msmjzv4Q.png"></div><figcaption class="imageCaption">This is the gradient descent <strong class="markup--strong markup--figure-strong">update rule</strong>. It tells us how to update the weights of our network to get us closer to the minimum we’re looking&nbsp;for.</figcaption></figure><p name="75e5" id="75e5" class="graf graf--p graf-after--figure">Let’s
 dissect. Every time we want to update our weights, we subtract the 
derivative of the cost function w.r.t. the weight itself, scaled by a <strong class="markup--strong markup--p-strong">learning rate</strong>&nbsp;,
 and — that’s it! You’ll see that as it gets closer and closer to the 
center, the derivative term gets smaller and smaller, converging to zero
 as it approaches the solution. The same process applies with networks 
that have tens, hundreds, thousands, or more parameters — compute the 
gradient of the cost function w.r.t. each of the weights, and update 
each of your weights accordingly.</p><p name="712f" id="712f" class="graf graf--p graf-after--p">I do want to say a few more words on the learning rate, because it’s one of the more important <strong class="markup--strong markup--p-strong">hyperparameters</strong>
 (“settings” for your neural network) that you have control over. If the
 learning rate is too high, it could jump too far in the other 
direction, and you never get to the minimum you’re searching for. Set it
 too low, and your network will take ages to find the right weights, or 
it will get stuck in a local minimum. There’s no “magic number” to use 
when it comes to a learning rate, and it’s usually best to try several 
and pick the one that works the best for your individual network and 
dataset. In practice, many choose to anneal the learning rate over 
time — it starts out high, because it’s furthest from the solution, and 
decays as it gets closer.</p><p name="27d8" id="27d8" class="graf graf--p graf-after--p">But
 as it turns out, gradient descent is kind of slow. Really slow, 
actually. Earlier I used the analogy of the weights “rolling” down the 
gradient to get to the bottom, but that doesn’t actually make any 
sense — it should pick up speed as it gets to the bottom, not slow down!
 Another iterative optimization algorithm, known as <strong class="markup--strong markup--p-strong">momentum</strong>,
 does just that. As the weights begin to “roll” down the slope, they 
pick up speed. When they get closer to the solution, the momentum that 
they picked up carries them closer to the optima while gradient descent 
would simply stop. As a result, training with momentum updates is both 
faster and can provide better results.</p><p name="e156" id="e156" class="graf graf--p graf-after--p">Here’s what the update rule looks like for momentum:</p><figure name="da54" id="da54" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 259px; max-height: 142px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.800000000000004%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*fO-eGqGcie5JQhaV5msoDw.png" data-width="259" data-height="142" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1fO-eGqGcie5JQhaV5msoDw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="40" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*fO-eGqGcie5JQhaV5msoDw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*fO-eGqGcie5JQhaV5msoDw.png"></noscript></div></div><figcaption class="imageCaption">You
 might see the momentum update rule written differently depending on 
where you look, but the basic principle remains the same throughout.</figcaption></figure><p name="3b50" id="3b50" class="graf graf--p graf-after--figure">As we train, we accumulate a “velocity” value <strong class="markup--strong markup--p-strong">V</strong>. At each training step, we update <strong class="markup--strong markup--p-strong">V</strong>
 with the gradient at the current position (once again scaled by the 
learning rate). Also notice that, with each time step, we decay velocity
 V by a factor <strong class="markup--strong markup--p-strong">mu</strong>
 (usually somewhere around&nbsp;.9), so that over time we lose momentum 
instead of bouncing around by the minimum forever. We then update our 
weight in the direction of the velocity, and repeat the process again. 
Over the first few training iterations, <strong class="markup--strong markup--p-strong">V</strong>
 will grow as our weights “pick up speed” and take successively bigger 
leaps. As we approach the minimum, our velocity stops accumulating as 
quickly, and eventually begins to decay, until we’ve essentially reached
 the minimum. An important thing to note is that we accumulate a 
velocity independently for each weight — just because one weight is 
changing particularly clearly doesn’t mean any of the other weights need
 to be.</p><p name="de0e" id="de0e" class="graf graf--p graf-after--p">There
 are lots of other iterative optimization algorithms that are commonly 
used with neural networks, but I won’t go into all of them here (if 
you’re curious, some of the more popular ones include <a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" data-href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf" class="markup--anchor markup--p-anchor">Adagrad</a> and <a href="http://arxiv.org/abs/1412.6980" data-href="http://arxiv.org/abs/1412.6980" class="markup--anchor markup--p-anchor">Adam</a>).
 The basic principle remains the same throughout — gradually update the 
weights to get them closer to the minimum. But regardless of which 
optimization algorithm you use, we still need to be able to compute the 
gradient of the cost function w.r.t. each weight. But our cost function 
isn’t a simple parabola anymore — it’s a complicated, many-dimensional 
function with countless local optima that we need to watch out for. 
That’s where backpropagation comes in.</p><h4 name="e456" id="e456" class="graf graf--h4 graf-after--p">Before Backpropagation</h4><p name="baac" id="baac" class="graf graf--p graf-after--h4">The
 backpropagation algorithm was a major milestone in machine learning 
because, before it was discovered, optimization methods were extremely 
unsatisfactory. One popular method was to perturb (adjust) the weights 
in a random, uninformed direction (ie. increase or decrease) and see if 
the performance of the ANN increased. If it did not, one would attempt 
to either a) go in the other direction b) reduce the perturbation size 
or c) a combination of both. Another attempt was to use Genetic 
Algorithms (which became popular in AI at the same time) to evolve a 
high-performance neural network. In both cases, without (analytically) 
being informed on the correct direction, results and efficiency were 
suboptimal. This is where the backpropagation algorithm comes into play.</p><h4 name="e6d2" id="e6d2" class="graf graf--h4 graf-after--p">The Backpropagation Algorithm</h4><p name="9613" id="9613" class="graf graf--p graf-after--h4">Recall
 that, for any given supervised machine learning problem, we (aim to) 
select weights that provide the most optimal estimation of a function 
that models our training data. In other words, we want to find a set of 
weights <strong class="markup--strong markup--p-strong">W</strong> that minimizes on the output of <strong class="markup--strong markup--p-strong">J(W)</strong>.
 We discussed the gradient descent algorithm — one where we update each 
weight by some negative, scalar reduction of the error derivative with 
respect to that weight. If we do choose to use gradient descent (or 
almost any other convex optimization algorithm), we need to find said 
derivatives in numerical form.</p><p name="1f67" id="1f67" class="graf graf--p graf-after--p">For
 other machine learning algorithms like logistic regression or linear 
regression, computing the derivatives is an elementary application of 
differentiation. This is because the outputs of these models are just 
the inputs multiplied by some chosen weights, and at most fed through a 
single activation function (the sigmoid function in logistic 
regression). The same, however, cannot be said for neural networks. To 
demonstrate this, here is a diagram of a single-layered, shallow neural 
network:</p><figure name="ee3f" id="ee3f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 641px; max-height: 138px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.5%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*Yk-ojFKLuPhZEHoU." data-width="641" data-height="138" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0Yk-ojFKLuPhZEHoU.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="15" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*Yk-ojFKLuPhZEHoU."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*Yk-ojFKLuPhZEHoU."></noscript></div></div></figure><p name="16ea" id="16ea" class="graf graf--p graf-after--figure">As
 you can see, each neuron is a function of the previous one connected to
 it. In other words, if one were to change the value of <strong class="markup--strong markup--p-strong">w1</strong>, both “hidden 1” <em class="markup--em markup--p-em">and </em>“hidden
 2” (and ultimately the output) neurons would change. Because of this 
notion of functional dependencies, we can mathematically formulate the 
output as an extensive composite function:</p><figure name="e44e" id="e44e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 341px; max-height: 124px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.4%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*vbSngK4VQd0QFWgL." data-width="341" data-height="124" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0vbSngK4VQd0QFWgL.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="25" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*vbSngK4VQd0QFWgL."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*vbSngK4VQd0QFWgL."></noscript></div></div></figure><p name="326e" id="326e" class="graf graf--p graf-after--figure">And thus:</p><figure name="643e" id="643e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 519px; max-height: 33px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 6.4%;"></div><img class="graf-image" data-image-id="0*MrJNRSWmwh8Yy-CG." data-width="519" data-height="33" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0MrJNRSWmwh8Yy-CG.png"></div></figure><p name="bea4" id="bea4" class="graf graf--p graf-after--figure">Here,
 the output is a composite function of the weights, inputs, and 
activation function(s). It is important to realize that the hidden 
units/nodes are simply intermediary computations that, in actuality, can
 be reduced down to computations of the input layer.</p><p name="ca87" id="ca87" class="graf graf--p graf-after--p">If we were to then take the derivative of said function with respect to some arbitrary weight (for example <strong class="markup--strong markup--p-strong">w1</strong>), we would iteratively apply the <a href="https://en.wikipedia.org/wiki/Chain_rule" data-href="https://en.wikipedia.org/wiki/Chain_rule" class="markup--anchor markup--p-anchor">chain rule</a> (which I’m sure you all remember from your calculus classes). The result would look similar to the following:</p><figure name="4ef6" id="4ef6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 65px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9.3%;"></div><img class="graf-image" data-image-id="0*9jzf-PRocsKHtPCt." data-width="784" data-height="73" data-action="zoom" data-action-value="0*9jzf-PRocsKHtPCt." draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/09jzf-PRocsKHtPCt.png"></div><figcaption class="imageCaption">If you fail to get an intuition of this, try researching about the chain&nbsp;rule.</figcaption></figure><p name="1191" id="1191" class="graf graf--p graf-after--figure">Now,
 let’s attach a black box to the tail of our neural network. This black 
box will compute and return the error — using the cost function — from 
our output:</p><figure name="2a52" id="2a52" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 134px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.1%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*SUsopH_FuWnAhQjD." data-width="752" data-height="144" data-action="zoom" data-action-value="0*SUsopH_FuWnAhQjD." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0SUsopH_FuWnAhQjD.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="12" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*SUsopH_FuWnAhQjD."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*SUsopH_FuWnAhQjD."></noscript></div></div></figure><p name="9bde" id="9bde" class="graf graf--p graf-after--figure">All
 we’ve done is add another functional dependency; our error is now a 
function of the output and hence a function of the input, weights, and 
activation function. If we were to compute the derivative of the error 
with any arbitrary weight (again, we’ll choose <strong class="markup--strong markup--p-strong">w1</strong>), the result would be:</p><figure name="b963" id="b963" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 649px; max-height: 79px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.2%;"></div><img class="graf-image" data-image-id="0*d5zULA4KVEsVYlng." data-width="649" data-height="79" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0d5zULA4KVEsVYlng.png"></div></figure><p name="4068" id="4068" class="graf graf--p graf-after--figure">Each
 of these derivatives can be simplified once we choose an activation and
 error function, such that the entire result would represent a numerical
 value. At that point, any abstraction has been removed, and the error 
derivative can be used in gradient descent (as discussed earlier) to 
iteratively improve upon the weight. We compute the error derivatives 
w.r.t. every other weight in the network and apply gradient descent in 
the same way. <em class="markup--em markup--p-em">This </em>is 
backpropagation — simply the computation of derivatives that are fed to a
 convex optimization algorithm. We call it “backpropagation” because it 
almost seems as if we are traversing from the output error to the 
weights, taking iterative steps using chain the rule until we “reach” 
our weight.</p><figure name="d24b" id="d24b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 134px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.1%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*k93UNqZ5FeScIVME." data-width="752" data-height="144" data-action="zoom" data-action-value="0*k93UNqZ5FeScIVME." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0k93UNqZ5FeScIVME.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="12" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*k93UNqZ5FeScIVME."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*k93UNqZ5FeScIVME."></noscript></div></div><figcaption class="imageCaption">It’s
 like feed-forward… but the opposite! We take the derivative of J w.r.t.
 the output, the output w.r.t. the hidden units, then the final hidden 
unit w.r.t. the&nbsp;weight.</figcaption></figure><p name="484a" id="484a" class="graf graf--p graf-after--figure">When
 I first truly understood the backprop algorithm (just a couple of weeks
 ago), I was taken aback by how simple it was. Sure, the actual 
arithmetic/computations can be difficult, but this process is handled by
 our computers. In reality, backpropagation is just a rather tedious 
(but again, for a generalized implementation, computers will handle 
this) application of the chain rule. Since neural networks are 
convoluted multilayer machine learning model structures (at least 
relative to other ones), each weight “contributes” to the overall error 
in a more complex manner, and hence the actual derivatives require a lot
 of effort to produce. However, once we get past the calculus, 
backpropagation of neural nets is equivalent to typical gradient descent
 for logistic/linear regression.</p><p name="4d68" id="4d68" class="graf graf--p graf-after--p">Thus
 far, I’ve walked through a very abstract form of backprop for a simple 
neural network. However, it is unlikely that you will ever use a 
single-layered ANN in applications. So, now, let’s make our black 
boxes — the activation and error functions — more concrete such that we 
can perform backprop on a multilayer neural net.</p><p name="06c4" id="06c4" class="graf graf--p graf-after--p">Recall that our error function <strong class="markup--strong markup--p-strong">J(W)</strong> will compute the “error” of our neural network based on the output predictions it produces vs. the correct <em class="markup--em markup--p-em">a priori</em> outputs we know in our training set. More formally, if we denote our predicted output estimations as vector <strong class="markup--strong markup--p-strong">p</strong>, and our actual output as vector <strong class="markup--strong markup--p-strong">a</strong>, then we can use:</p><figure name="7895" id="7895" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 273px; max-height: 73px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.700000000000003%;"></div><img class="graf-image" data-image-id="1*E1puoFN7OGdpwtxC2BxNfw.png" data-width="273" data-height="73" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1E1puoFN7OGdpwtxC2BxNfw.png"></div><figcaption class="imageCaption">In this case, <strong class="markup--strong markup--figure-strong">J </strong>need not be a function of <strong class="markup--strong markup--figure-strong">W </strong>because <strong class="markup--strong markup--figure-strong">p </strong>already is. We can use vector notation here because the inputs/outputs to our neural nets our vectors/some form of&nbsp;tensor.</figcaption></figure><p name="85f3" id="85f3" class="graf graf--p graf-after--figure">This is just one example of a possible cost function (the <a href="https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood" data-href="https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood" class="markup--anchor markup--p-anchor">log-likelihood</a>
 is also a popular one), and we use it because of its mathematical 
convenience (this is a notion one will frequently encounter in machine 
learning): the squared expression exaggerates poor solutions and ensures
 each discrepancy is positive. It will soon become clear why we multiply
 the expression by half.</p><p name="ef2d" id="ef2d" class="graf graf--p graf-after--p">The
 derivative of the error w.r.t. the output was the first term in the 
error w.r.t. weight derivative expression we formulated earlier. Let’s 
now compute it!</p><figure name="bdeb" id="bdeb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 574px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15%;"></div><img class="graf-image" data-image-id="1*4WviflgVzi6jLFd31xfH5w.png" data-width="574" data-height="86" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/14WviflgVzi6jLFd31xfH5w.png"></div><figcaption class="imageCaption">With
 a simple application of the power and chain rule, our derivative is 
complete. The half gets cancelled due to the power&nbsp;rule.</figcaption></figure><p name="f319" id="f319" class="graf graf--p graf-after--figure">Our result is simply our predictions take away our actual outputs.</p><p name="9591" id="9591" class="graf graf--p graf-after--p">Now,
 let’s move on to the activation function. The activation function used 
depends on the context of the neural network. If we aren’t in a 
classification context, ReLU (Rectified Linear Unit, which is zero if 
input is negative, and the identity function when the input is positive)
 is commonly used today.</p><figure name="6d90" id="6d90" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 528px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75.4%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="1*I3m4X5zMlOhhK8W3j3Pvkw.png" data-width="812" data-height="612" data-action="zoom" data-action-value="1*I3m4X5zMlOhhK8W3j3Pvkw.png" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1I3m4X5zMlOhhK8W3j3Pvkw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="55" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/1*I3m4X5zMlOhhK8W3j3Pvkw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/1*I3m4X5zMlOhhK8W3j3Pvkw.png"></noscript></div></div><figcaption class="imageCaption">The “Rectified Linear Unit” acivation function — <a href="http://i.stack.imgur.com/8CGlM.png" data-href="http://i.stack.imgur.com/8CGlM.png" class="markup--anchor markup--figure-anchor" rel="nofollow">http://i.stack.imgur.com/8CGlM.png</a></figcaption></figure><p name="2aab" id="2aab" class="graf graf--p graf-after--figure">If
 we’re in a classification context (that is, predicting on a discrete 
state with a probability ie. if an email is spam), we can use the 
sigmoid or tanh (hyperbolic tangent) function such that we can “squeeze”
 any value into the range 0 to 1. These are used instead of a typical <a href="https://en.wikipedia.org/wiki/Step_function" data-href="https://en.wikipedia.org/wiki/Step_function" class="markup--anchor markup--p-anchor">step function</a> because their “smoothness” properties allows for the derivatives to be non-zero. The derivative of the step function before <strong class="markup--strong markup--p-strong">and </strong>after the origin is zero. This will pose issues when we try to update our weights (nothing much will happen!).</p><p name="818c" id="818c" class="graf graf--p graf-after--p">Now, let’s say we’re in a classification context and we choose to use the sigmoid function, which is of the following equation:</p><figure name="4672" id="4672" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 317px; max-height: 76px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24%;"></div><img class="graf-image" data-image-id="0*vT63OsGDWkOs3W8z." data-width="317" data-height="76" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0vT63OsGDWkOs3W8z.png"></div></figure><figure name="90ec" id="90ec" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 593px; max-height: 225px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.9%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*oaeR_qLU8v_oNr3G." data-width="593" data-height="225" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0oaeR_qLU8v_oNr3G.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="27" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*oaeR_qLU8v_oNr3G."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*oaeR_qLU8v_oNr3G."></noscript></div></div><figcaption class="imageCaption">Smooth, continuous sigmoid function on the left. Step, piece-wise function on the right. <a href="https://en.wikibooks.org/wiki/File:HardLimitFunction.png" data-href="https://en.wikibooks.org/wiki/File:HardLimitFunction.png" class="markup--anchor markup--figure-anchor">https://en.wikibooks.org/wiki/File:HardLimitFunction.png</a> &amp; <a href="https://en.wikibooks.org/wiki/File:SigmoidFunction.png" data-href="https://en.wikibooks.org/wiki/File:SigmoidFunction.png" class="markup--anchor markup--figure-anchor">https://en.wikibooks.org/wiki/File:SigmoidFunction.png</a>.</figcaption></figure><p name="7f19" id="7f19" class="graf graf--p graf-after--figure">As per usual, we’ll compute the derivative using differentiation rules as:</p><figure name="4560" id="4560" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 189px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*2dW7Dvyhiop3GaVN." data-width="800" data-height="216" data-action="zoom" data-action-value="0*2dW7Dvyhiop3GaVN." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/02dW7Dvyhiop3GaVN.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="20" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*2dW7Dvyhiop3GaVN."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*2dW7Dvyhiop3GaVN."></noscript></div></div><figcaption class="imageCaption">Looks
 confusing? Forgot differentiation? Don’t worry! Just take my word for 
it. It’s not necessary to have a complete mathematical comprehension of 
this derivation.</figcaption></figure><p name="ba16" id="ba16" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Sidenote:
 ReLU activation functions are also commonly used in classification 
contexts. There are downsides to using the sigmoid 
function — particularly the “vanishing gradient” problem — which you can
 read more about </strong><a href="https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning" data-href="https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning" class="markup--anchor markup--p-anchor"><strong class="markup--strong markup--p-strong">here</strong></a><strong class="markup--strong markup--p-strong">.</strong></p><p name="f152" id="f152" class="graf graf--p graf-after--p">The
 sigmoid function is mathematically convenient (there it is again!) 
because we can represent its derivative in terms of the output of the 
function. Isn’t that cool‽</p><p name="dd9b" id="dd9b" class="graf graf--p graf-after--p">We
 are now in a good place to perform backpropagation on a multilayer 
neural network. Let me introduce you to the net we are going to work 
with:</p><figure name="2060" id="2060" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.099999999999994%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*m0tn6W0ipC3Ro7mo." data-width="768" data-height="362" data-action="zoom" data-action-value="0*m0tn6W0ipC3Ro7mo." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0m0tn6W0ipC3Ro7mo_002.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="35" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*m0tn6W0ipC3Ro7mo."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*m0tn6W0ipC3Ro7mo."></noscript></div></div></figure><p name="89af" id="89af" class="graf graf--p graf-after--figure">This
 net is still not as complex as one you may use in your programming, but
 its architecture allows us to nevertheless get a good grasp on 
backprop. In this net, we have 3 input neurons and one output neuron. 
There are four layers in total: one input, one output, and two hidden 
layers. There are 3 neurons in each hidden layer, too (which, by the 
way, need not be the case). The network is fully connected; there are no
 missing connections. Each neuron/node (save the inputs, which are 
usually pre-processed anyways) is an <strong class="markup--strong markup--p-strong">activity</strong>; it is the weighted sum of the previous neurons’ activities applied to the sigmoid activation function.</p><p name="29a6" id="29a6" class="graf graf--p graf-after--p">To perform backprop by hand, we need to introduce the different variables/states at <em class="markup--em markup--p-em">each </em>point (layer-wise) in the neural network:</p><figure name="b3a9" id="b3a9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 282px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.2%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*JrJK5fbLAd5zum6h." data-width="907" data-height="365" data-action="zoom" data-action-value="0*JrJK5fbLAd5zum6h." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0JrJK5fbLAd5zum6h.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="30" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*JrJK5fbLAd5zum6h."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*JrJK5fbLAd5zum6h."></noscript></div></div></figure><p name="41d2" id="41d2" class="graf graf--p graf-after--figure">It is important to note that every variable you see here is a <strong class="markup--strong markup--p-strong">generalization </strong>on the entire layer at that point. For example, when I say <strong class="markup--strong markup--p-strong">x_i</strong>, I am referring to the input to any input neuron (arbitrary value of <strong class="markup--strong markup--p-strong">i</strong>). I chose to place it in the middle of the layer for visibility purposes, but that does <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">not</em></strong> mean that <strong class="markup--strong markup--p-strong">x_i</strong> refers to the middle neuron. I’ll demonstrate and discuss the implications of this later on.</p><p name="17bc" id="17bc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">x </strong>refers to the input layer, <strong class="markup--strong markup--p-strong">y </strong>refers to hidden layer 1, <strong class="markup--strong markup--p-strong">z </strong>refers to hidden layer 2, and <strong class="markup--strong markup--p-strong">p </strong>refers
 to the prediction/output layer (which fits in nicely with the notation 
used in our cost function). If a variable has the subscript <strong class="markup--strong markup--p-strong">i</strong>, it means that the variable is the <em class="markup--em markup--p-em">input </em>to the relevant neuron at that layer. If a variable has the subscript <strong class="markup--strong markup--p-strong">j</strong>, it means that the variable is the <em class="markup--em markup--p-em">output</em> of the relevant neuron at that layer. For example, <strong class="markup--strong markup--p-strong">x_i </strong>refers to any input value we enter into the network. <strong class="markup--strong markup--p-strong">x_j</strong> is actually equal to <strong class="markup--strong markup--p-strong">x_i</strong>,
 but this is only because we choose not to use an activation 
function — or rather, we use the identity activation function — in the 
input layer’s activities. We only include these two separate variables 
to retain consistency. <strong class="markup--strong markup--p-strong">y_i </strong>is
 the input to any neuron in the first hidden layer; it is the weighted 
sum of all previous neurons (each neuron in the input layer multiplied 
by the corresponding connecting weights). <strong class="markup--strong markup--p-strong">y_j </strong>is the output of any neuron at the hidden layer, so it is equal to <strong class="markup--strong markup--p-strong">activation_function(y_i) = sigmoid(y_i) = sigmoid(weighted_sum_of_x_j)</strong>. We can apply the same logic for <strong class="markup--strong markup--p-strong">z </strong>and <strong class="markup--strong markup--p-strong">p</strong>. Ultimately, <strong class="markup--strong markup--p-strong">p_j </strong>is the sigmoid output of <strong class="markup--strong markup--p-strong">p_i </strong>and hence is the output of the entire neural network that we pass to the error/cost function.</p><p name="cf41" id="cf41" class="graf graf--p graf-after--p">The weights are organized into three separate variables: <strong class="markup--strong markup--p-strong">W1</strong>, <strong class="markup--strong markup--p-strong">W2</strong>, and <strong class="markup--strong markup--p-strong">W3</strong>. Each <strong class="markup--strong markup--p-strong">W </strong>is
 a matrix (if you are not comfortable with Linear Algebra, think of a 2D
 array) of all the weights at the given layer. For example, <strong class="markup--strong markup--p-strong">W1 </strong>are the weights that connect the input layer to the hidden layer 1. <strong class="markup--strong markup--p-strong">Wlayer_ij </strong>refers to any arbitrary, single weight at a given layer. To get an intuition of <strong class="markup--strong markup--p-strong">ij </strong>(which is really <strong class="markup--strong markup--p-strong">i, j</strong>), <strong class="markup--strong markup--p-strong">Wlayer_i </strong>are all the weights that connect arbitrary neuron <strong class="markup--strong markup--p-strong">i </strong>at a given layer to the next layer. <strong class="markup--strong markup--p-strong">Wlayer_ij </strong>(adding the <strong class="markup--strong markup--p-strong">j </strong>component) is the weight that connects arbitrary neuron <strong class="markup--strong markup--p-strong">i </strong>at a given layer to an arbitrary neuron <strong class="markup--strong markup--p-strong">j </strong>at the next layer. Essentially, <strong class="markup--strong markup--p-strong">Wlayer </strong>is a vector of <strong class="markup--strong markup--p-strong">Wlayer_i</strong>s, which is a vector of real-valued <strong class="markup--strong markup--p-strong">Wlayer_ij</strong>s.</p><p name="964e" id="964e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">NOTE: </strong>Please note that the <strong class="markup--strong markup--p-strong">i</strong>’s<strong class="markup--strong markup--p-strong"> </strong>and <strong class="markup--strong markup--p-strong">j</strong>’s in the weights and other variables are <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">completely different</em></strong>. These indices do not correspond in any way. In fact, for <strong class="markup--strong markup--p-strong">x/y/z/p</strong>, <strong class="markup--strong markup--p-strong">i </strong>and <strong class="markup--strong markup--p-strong">j </strong>do not represent tensor indices at all, they simply represent the input and output of a neuron. <strong class="markup--strong markup--p-strong">Wlayer_ij </strong>represents an arbitrary weight at an index in a weight matrix, and <strong class="markup--strong markup--p-strong">x_j/y_j/z_j/p_j </strong>represent an arbitrary input/output point of a neuron unit.</p><p name="59cc" id="59cc" class="graf graf--p graf-after--p">That
 last part about weights was tedious! It’s crucial to understand how 
we’re separating the neural network here, especially the notion of 
generalizing on an entire layer, before moving forward.</p><p name="6a70" id="6a70" class="graf graf--p graf-after--p">To
 acquire a comprehensive intuition of backpropagation, we’re going to 
backprop this neural net as discussed before. More specifically, we’re 
going to find the <strong class="markup--strong markup--p-strong">derivative of the error w.r.t. an arbitrary weight in the input layer </strong>(<strong class="markup--strong markup--p-strong">W1_ij</strong>).
 We could find the derivative of the error w.r.t. an arbitrary weight in
 the first or second hidden layer, but let’s go as far back as we can; 
the more backprop, the better!</p><p name="c6dd" id="c6dd" class="graf graf--p graf-after--p">So, mathematically, we are trying to obtain (to perform our iterative optimization algorithm with):</p><figure name="4412" id="4412" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 196px; max-height: 87px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 44.4%;"></div><img class="graf-image" data-image-id="0*QMwl-OVNNQ8KZVTv." data-width="196" data-height="87" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0QMwl-OVNNQ8KZVTv.png"></div></figure><p name="9dd3" id="9dd3" class="graf graf--p graf-after--figure">We can express this graphically/visually, using the same principles as earlier (chain rule), like so:</p><figure name="83d0" id="83d0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 282px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.2%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*HLo6XxRwCzjUzXXh." data-width="907" data-height="365" data-action="zoom" data-action-value="0*HLo6XxRwCzjUzXXh." draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0HLo6XxRwCzjUzXXh.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="30" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*HLo6XxRwCzjUzXXh."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*HLo6XxRwCzjUzXXh."></noscript></div></div><figcaption class="imageCaption">We backpropagate from the error all the way to the weights in the input layer. Note that the weight itself is any <strong class="markup--strong markup--figure-strong">arbitrary</strong> one in that layer. In a <strong class="markup--strong markup--figure-strong">fully connected </strong>neural net, we can make these generalizations considering we are consistent with our&nbsp;indices.</figcaption></figure><p name="e852" id="e852" class="graf graf--p graf-after--figure">In
 two layers, we have three red lines pointing in three different 
directions, instead of just one. This is a reinforcement of (and why it 
is important to understand) the fact that <strong class="markup--strong markup--p-strong">variable j </strong>is just a generalization/represents any point in the layer. So, when we differentiate <strong class="markup--strong markup--p-strong">p_i</strong> with respect to the layer before that, there are <strong class="markup--strong markup--p-strong">three different weights</strong>, as I hope you can see, in <strong class="markup--strong markup--p-strong">W3_ij</strong> that contribute to the value <strong class="markup--strong markup--p-strong">p_i</strong>. There also happen to be three weights in <strong class="markup--strong markup--p-strong">W3</strong> in total, but this isn’t the case for the layers before; it is only the case because layer <strong class="markup--strong markup--p-strong">p </strong>has
 one neuron — the output — in it. We stop backprop at the input layer 
and so we just point to the single weight we are looking for.</p><p name="3587" id="3587" class="graf graf--p graf-after--p">Wonderful! Now let’s work out all this great stuff mathematically. Immediately, we know:</p><figure name="bee9" id="bee9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 357px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.1%;"></div><img class="graf-image" data-image-id="0*DQcoyR-fFf2ieOBo." data-width="357" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0DQcoyR-fFf2ieOBo.png"></div></figure><p name="29fa" id="29fa" class="graf graf--p graf-after--figure">We
 have already established the left hand side, so now we just need to use
 the chain rule to simplify it further. The derivative of the error 
w.r.t. the weight can be written as the derivative of the error w.r.t. 
the output prediction multiplied by the derivative of the output 
prediction w.r.t. the weight. At this point, we’ve traversed one red 
line back. We know this because</p><figure name="a783" id="a783" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 56px; max-height: 86px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 153.6%;"></div><img class="graf-image" data-image-id="0*vRuT30CH_1RLY7MQ." data-width="56" data-height="86" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0vRuT30CH_1RLY7MQ.png"></div></figure><p name="d8d2" id="d8d2" class="graf graf--p graf-after--figure">is reducible to a numerical value. Specifically, the derivative of the error w.r.t. the output prediction is:</p><figure name="1d98" id="1d98" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 99px; max-height: 53px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 53.5%;"></div><img class="graf-image" data-image-id="1*uVimZGqI0FPVXXb3oe9BWw.png" data-width="99" data-height="53" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1uVimZGqI0FPVXXb3oe9BWw.png"></div><figcaption class="imageCaption">We know this from our manual derivation&nbsp;earlier.</figcaption></figure><p name="5256" id="5256" class="graf graf--p graf-after--figure">Hence:</p><figure name="05af" id="05af" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 279px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.300000000000004%;"></div><img class="graf-image" data-image-id="1*nZwutmypOiiEooG8kxorKg.png" data-width="279" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1nZwutmypOiiEooG8kxorKg.png"></div></figure><p name="88c4" id="88c4" class="graf graf--p graf-after--figure">Going one more layer backwards, we can determine that:</p><figure name="efaf" id="efaf" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 235px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.6%;"></div><img class="graf-image" data-image-id="0*94LsJGVc_g4q66-U." data-width="235" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/094LsJGVc_g4q66-U.png"></div></figure><p name="ecae" id="ecae" class="graf graf--p graf-after--figure">In
 other words, the derivative of the output prediction w.r.t. the weight 
is the derivative of the output w.r.t. the input to the output layer (<strong class="markup--strong markup--p-strong">p_i</strong>)
 multiplied by the derivative of that value w.r.t. the weight. This 
represents our second red line. We can solve the first term like so:</p><figure name="9739" id="9739" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 211px; max-height: 88px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.699999999999996%;"></div><img class="graf-image" data-image-id="0*ggH61s46zIpxtLV_." data-width="211" data-height="88" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0ggH61s46zIpxtLV_.png"></div></figure><p name="da49" id="da49" class="graf graf--p graf-after--figure">This
 corresponds with the derivative of the sigmoid function we solved 
earlier, which was equal to the output multiplied by one minus the 
output. In this case, <strong class="markup--strong markup--p-strong">p_j </strong><em class="markup--em markup--p-em">is </em>the output of the sigmoid function. Now, we have:</p><figure name="4a10" id="4a10" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 303px; max-height: 131px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.2%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*prpHixS0PhAhxjG3." data-width="303" data-height="131" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0prpHixS0PhAhxjG3.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="30" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*prpHixS0PhAhxjG3."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*prpHixS0PhAhxjG3."></noscript></div></div></figure><figure name="b277" id="b277" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 441px; max-height: 89px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.200000000000003%;"></div><img class="graf-image" data-image-id="1*fGy8zY6O7zWES2IS7wXu_g.png" data-width="441" data-height="89" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1fGy8zY6O7zWES2IS7wXu_g.png"></div></figure><p name="3e70" id="3e70" class="graf graf--p graf-after--figure">Let’s
 move on to the third red line(s). This one is interesting because we 
begin to “spread” out. Since there are multiple different weights that 
contribute to the value of <strong class="markup--strong markup--p-strong">p_i</strong>, we need to take into account their individual “pull” factors into our derivative:</p><figure name="7455" id="7455" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 272px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.2%;"></div><img class="graf-image" data-image-id="0*N0RhnqpTNMF3t_1B." data-width="272" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0N0RhnqpTNMF3t_1B.png"></div></figure><p name="e609" id="e609" class="graf graf--p graf-after--figure">If
 you’re a mathematician, this notation may irk you slightly; sorry if 
that’s the case! In computer science, we tend to stray from the notion 
of completely legal mathematical expressions. This is yet again again 
another reason why it’s key to understand the role of layer 
generalization; <strong class="markup--strong markup--p-strong">z_j</strong> here is not just referring to the middle neuron, it’s referring to an arbitrary neuron. The actual <em class="markup--em markup--p-em">value </em>of <strong class="markup--strong markup--p-strong">j</strong>
 in the summation is not changing (it’s not even an index or a value in 
the first place), and we don’t really consider it. It’s less of a 
mathematical expression and more of a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">statement</em></strong> that we will iterate through each generalized neuron <strong class="markup--strong markup--p-strong">z_j </strong>and use it. In other words, we iterate over the derivative terms and sum them up using <strong class="markup--strong markup--p-strong">z_1</strong>, <strong class="markup--strong markup--p-strong">z_2</strong>, and <strong class="markup--strong markup--p-strong">z_3</strong>. Before, we could write <strong class="markup--strong markup--p-strong">p_j </strong>as any single value because the output layer just contains one node; there is just one <strong class="markup--strong markup--p-strong">p_j</strong>. But we see here that this is no longer the case. We have multiple <strong class="markup--strong markup--p-strong">z_j</strong> values, and<strong class="markup--strong markup--p-strong"> p_i</strong> is functionally dependent on each of these <strong class="markup--strong markup--p-strong">z_j</strong> values. So, when we traverse from<strong class="markup--strong markup--p-strong"> p_j</strong> to the preceding layer, we need to consider each contribution from layer <strong class="markup--strong markup--p-strong">z</strong> to <strong class="markup--strong markup--p-strong">p_j</strong>
 separately and add them up to create one total contribution. There’s no
 upper bound to the summation; we just assume that we start at zero and 
end at our maximum value for the number of neurons in the layer. Please 
again note that the same changes are not reflected in <strong class="markup--strong markup--p-strong">W1_ij</strong>, where <strong class="markup--strong markup--p-strong">j</strong> refers to an entirely different thing.<strong class="markup--strong markup--p-strong"> </strong>Instead, we’re just <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">stating </em></strong>that we will use the different <strong class="markup--strong markup--p-strong">z_j </strong>neurons in layer <strong class="markup--strong markup--p-strong">z</strong>.</p><p name="cdf7" id="cdf7" class="graf graf--p graf-after--p">Since <strong class="markup--strong markup--p-strong">p_j</strong> is a summation of each weight multiplied by each <strong class="markup--strong markup--p-strong">z_j </strong>(weighted sum), if we were to take the derivative of <strong class="markup--strong markup--p-strong">p_j </strong>with any arbitrary <strong class="markup--strong markup--p-strong">z_j</strong>,
 the result would be the connecting weight since said weight would be 
the coefficient of the term (derivative of m*x w.r.t. x is just m):</p><figure name="cd24" id="cd24" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 134px; max-height: 88px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.7%;"></div><img class="graf-image" data-image-id="0*nGZuc0J3Wc6oZWmn." data-width="134" data-height="88" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0nGZuc0J3Wc6oZWmn.png"></div></figure><p name="0ef7" id="0ef7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">W3_ij</strong> is loosely defined here. <strong class="markup--strong markup--p-strong">ij </strong>still refers to any arbitrary weight — where <strong class="markup--strong markup--p-strong">ij </strong>are still separate from the <strong class="markup--strong markup--p-strong">j </strong>used in <strong class="markup--strong markup--p-strong">p_i/z_j</strong> 
— but again, as computer scientists and not mathematicians, we need not 
be pedantic about the legality and intricacy of expressions; we just 
need an intuition of what the expressions imply/mean. It’s almost a 
succinct form of psuedo-code! So, even though this defines an arbitrary 
weight, we know it means the <strong class="markup--strong markup--p-strong">connecting </strong>weight. We can also see this from the diagram: when we walk from <strong class="markup--strong markup--p-strong">p_j </strong>to an arbitrary <strong class="markup--strong markup--p-strong">z_j</strong>, we walk along the connecting weight. So now, we have:</p><figure name="530e" id="530e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 250px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.2%;"></div><img class="graf-image" data-image-id="0*w-inVp_X6HASLTdv." data-width="250" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0w-inVp_X6HASLTdv.png"></div></figure><p name="dcb9" id="dcb9" class="graf graf--p graf-after--figure">At
 this point, I like to continue playing the “reduction test”. The 
reduction test states that, if we can further simplify a derivative 
term, we still have more backprop to do. Since we can’t yet quite put 
the derivative of <strong class="markup--strong markup--p-strong">z_j</strong> w.r.t. <strong class="markup--strong markup--p-strong">W1_ij</strong>
 into a numerical term, let’s keep going (and fast-forward a bit). Using
 chain rule, we follow the fourth line back to determine that:</p><figure name="4d77" id="4d77" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 469px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.8%;"></div><img class="graf-image" data-image-id="0*SdmtQirS-Dj_O77y." data-width="469" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0SdmtQirS-Dj_O77y.png"></div></figure><p name="7386" id="7386" class="graf graf--p graf-after--figure">Since <strong class="markup--strong markup--p-strong">z_j </strong>is the sigmoid of <strong class="markup--strong markup--p-strong">z_i</strong>, we use the same logic as the previous layer and apply the sigmoid derivative. The derivative of <strong class="markup--strong markup--p-strong">z_i </strong>w.r.t. <strong class="markup--strong markup--p-strong">W1_ij</strong>, demonstrated by the fifth line(s) back, requires the same idea of “spreading out” and summation of contributions:</p><figure name="627e" id="627e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 468px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.900000000000002%;"></div><img class="graf-image" data-image-id="0*KpJrA6EpVWN2Eo2z." data-width="468" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0KpJrA6EpVWN2Eo2z.png"></div></figure><p name="e9fc" id="e9fc" class="graf graf--p graf-after--figure">Briefly, since <strong class="markup--strong markup--p-strong">z_i </strong>is the weighted sum of each <strong class="markup--strong markup--p-strong">y_j </strong>in <strong class="markup--strong markup--p-strong">y</strong>, we sum over the derivatives which, similar to before, simplifies to the relevant connecting weights in the preceding layer (<strong class="markup--strong markup--p-strong">W2</strong> in this case).</p><p name="71de" id="71de" class="graf graf--p graf-after--p">We’re almost there, let’s go further; there’s still more reduction to do:</p><figure name="b3f1" id="b3f1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 459px; max-height: 93px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.3%;"></div><img class="graf-image" data-image-id="0*ia5ppPK0yG1f-8WQ." data-width="459" data-height="93" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0ia5ppPK0yG1f-8WQ.png"></div></figure><p name="3673" id="3673" class="graf graf--p graf-after--figure">We
 have, of course, another sigmoid activation function to deal with. This
 is the sixth red line. Notice, now, that we have just <em class="markup--em markup--p-em">one </em>line
 remaining. In fact, our last derivative term here passes (or rather, 
fails) the reduction test! The last line traverses from the input at <strong class="markup--strong markup--p-strong">y_i </strong>to <strong class="markup--strong markup--p-strong">x_j</strong>, walking along <strong class="markup--strong markup--p-strong">W1_ij</strong>. Wait a second — is this not what we are attempting to backprop to? Yes, it is! Since we are, for the first time, <em class="markup--em markup--p-em">directly </em>deriving <strong class="markup--strong markup--p-strong">y_i</strong> w.r.t. the weight <strong class="markup--strong markup--p-strong">W1_ij</strong>, we can think of the coefficient of <strong class="markup--strong markup--p-strong">W1_ij </strong>as being <strong class="markup--strong markup--p-strong">x_j </strong>in our weighted sum (instead of the vice versa as used previously). Hence, the simplification follows:</p><figure name="b748" id="b748" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 175px; max-height: 89px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.9%;"></div><img class="graf-image" data-image-id="0*GfdjMI9YPac0G9Ix." data-width="175" data-height="89" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0GfdjMI9YPac0G9Ix.png"></div></figure><p name="706b" id="706b" class="graf graf--p graf-after--figure">Of course, since each <strong class="markup--strong markup--p-strong">x_j </strong>in layer <strong class="markup--strong markup--p-strong">x </strong>contributes to the weighted sum <strong class="markup--strong markup--p-strong">y_i</strong>,
 we sum over the effects. And that’s it! We can’t reduce any further 
from here. Now, let’s tie all these individual expressions together:</p><figure name="5da6" id="5da6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 66px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 9.5%;"></div><img class="graf-image" data-image-id="1*UY2yJCSfx5y-zzlNAIWrSQ.png" data-width="993" data-height="94" data-action="zoom" data-action-value="1*UY2yJCSfx5y-zzlNAIWrSQ.png" draggable="false" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1UY2yJCSfx5y-zzlNAIWrSQ.png"></div><figcaption class="imageCaption">Our final expression for the derivative of the error w.r.t. any weight in&nbsp;<strong class="markup--strong markup--figure-strong">W1</strong></figcaption></figure><p name="5cb5" id="5cb5" class="graf graf--p graf-after--figure">With
 no more partial derivative terms left, our work is complete! This gives
 us the derivative of the error w.r.t. any arbitrary weight in the input
 layer/<strong class="markup--strong markup--p-strong">W1</strong>. That was a lot of work — maybe now we can sympathize with the poor computers!</p><p name="ad17" id="ad17" class="graf graf--p graf-after--p">Something you should notice is that values such as <strong class="markup--strong markup--p-strong">p_j, a, z_j, y_j, x_j </strong>etc.
 are the values of the network at the different points. But where do 
they come from? Actually, we would need to perform a feed-forward of the
 neural network first and then capture these variables.</p><h4 name="eefb" id="eefb" class="graf graf--h4 graf-after--p">Neural Network Training&nbsp;Overview</h4><p name="abbb" id="abbb" class="graf graf--p graf-after--h4">Our task is to now perform Gradient Descent to train the neural net:</p><figure name="e232" id="e232" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 417px; max-height: 183px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.9%;"></div><div data-scroll="native" class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded" data-image-id="0*B_2wAzU14ush5uBO." data-width="417" data-height="183" draggable="false"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/0B_2wAzU14ush5uBO.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas height="32" width="75" class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/800/0*B_2wAzU14ush5uBO."><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/800/0*B_2wAzU14ush5uBO."></noscript></div></div></figure><p name="858b" id="858b" class="graf graf--p graf-after--figure">We
 perform gradient descent on each weight in each layer. Notice that the 
resulting gradient should change each time because the weight itself 
changes, (and as a result, the performance and output of the entire net 
should change) even if it’s a small perturbation. This means that, at 
each update, we need to do a feed-forward of the neural net. Not just 
once before, but once <em class="markup--em markup--p-em">each iteration</em>.</p><p name="9d72" id="9d72" class="graf graf--p graf-after--p">These are then the steps to train an entire neural network:</p><ol class="postList"><li name="1433" id="1433" class="graf graf--li graf-after--p">Create our connected neural network and prepare training data</li><li name="83ad" id="83ad" class="graf graf--li graf-after--li">Initialize all the weights to random values</li></ol><p name="6518" id="6518" class="graf graf--p graf-after--li">It’s
 important to note that one must not initialize the weights to zero, 
similar to what may be done in other machine learning algorithms. If 
weights <em class="markup--em markup--p-em">are </em>initialized to 
zero, after each update, the outgoing weights of each neuron will be 
identical, because the gradients will be identical (which can be 
proved). Because of this, the proceeding hidden units will remain the 
same value and will continue to follow each other. Ultimately, this 
means that our training will become extremely constrained (due to the 
“symmetry”), and we won’t be able to build interesting functions. Also, 
neural networks may get stuck at local optima (places where the gradient
 is zero but are not the global minima), so random weight initialization
 allows one to hopefully have a chance of circumventing that by starting
 at many different random values.</p><p name="c8d5" id="c8d5" class="graf graf--p graf-after--p">3. Perform one feed-forward using the training data</p><p name="248d" id="248d" class="graf graf--p graf-after--p">4. Perform backpropagation to get the error derivatives w.r.t. each and every weight in the neural network</p><p name="e96a" id="e96a" class="graf graf--p graf-after--p">5.
 Perform gradient descent to update each weight by the negative scalar 
reduction (w.r.t. some learning rate alpha) of the respective error 
derivative. Increment the number of iterations.</p><p name="1d1d" id="1d1d" class="graf graf--p graf-after--p">6.
 If we have converged (in reality, though, we just stop when we have 
reached the number of maximum iterations) training is complete. Else, 
repeat starting at step 3.</p><p name="26b0" id="26b0" class="graf graf--p graf-after--p">If
 we initialize our weights randomly (and not to zero) and then perform 
gradient descent with derivatives computed from backpropagation, we 
should expect to train a neural network in no time! I hope this example 
brought clarity to how backprop works and the intuition behind it. If 
you didn’t understand the intricacies of the example but understand and 
appreciate the concept of backprop as a whole, you’re still in a great 
place! Next we’ll go ahead and explain backprop code that works on any 
generalized architecture of a neural network using the ReLU activation 
function.</p><h4 name="ef5a" id="ef5a" class="graf graf--h4 graf-after--p">Implementing Backpropagation</h4><p name="6a97" id="6a97" class="graf graf--p graf-after--h4">Now
 that we’ve developed the math and intuition behind backpropagation, 
let’s try to implement it. We’ll divide our implementation into three 
distinct steps:</p><ol class="postList"><li name="5493" id="5493" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Feed-forward</strong>.
 In this step, we take the inputs and forward them through the network, 
layer by layer, to generate the output activations (as well as all of 
the activations in the hidden layers). When we are actually using our 
network (rather than training it), this is the only step we’ll need to 
perform.</li><li name="2aa2" id="2aa2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Backpropagation</strong>.
 Here, we’ll take our error function and compute the weight gradients 
for each layer. We’ll use the algorithm just described to compute the 
derivative of the cost function w.r.t. each of the weights in our 
network, which will in turn allow us to complete step 3.</li><li name="e69d" id="e69d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Weight update</strong>.
 Finally, we’ll use the gradients computed in step 2 to update our 
weights. We can use any of the update rules discussed previously during 
this step (gradient descent, momentum, and so on).</li></ol><p name="88ea" id="88ea" class="graf graf--p graf-after--li">Let’s
 start off by defining what the API we’re implementing looks like. We’ll
 define our network as a series of Layer instances that our data passes 
through — this means that instead of modeling each individual neuron, we
 group neurons from a single layer together. This makes it a bit easier 
to reason about larger networks, but also makes the actual computations 
faster (as we’ll see shortly). Also — we’re going to write the code in 
Python.</p><p name="6860" id="6860" class="graf graf--p graf-after--p">Each layer will have the following API:</p><pre name="d16f" id="d16f" class="graf graf--pre graf-after--p">class ReLULayer(object):<br>    def __init__(self, size_in, size_out):<br>        # Initialize this layer with random weights and any other  <br>        # parameters we may need.<br>        pass</pre><pre name="4476" id="4476" class="graf graf--pre graf-after--pre">    def forward(self, in_act):<br>        # Compute the outgoing activations from this layer, given <br>        # the activations from the previous layer.<br>        pass</pre><pre name="3b56" id="3b56" class="graf graf--pre graf-after--pre">    def backward(self, out_grad):<br>        # out_grad is the derivative of the cost function w.r.t. the <br>        # inputs to all of the neurons for the following layer. We <br>        # need to compute the gradient of our own weights, and <br>        # return another the gradient of the inputs to this layer to <br>        # continue the backpropagation.<br>        pass</pre><pre name="dcbd" id="dcbd" class="graf graf--pre graf-after--pre">    def update(self, alpha):<br>        # Perform the actual weight update step.<br>        pass</pre><p name="26ef" id="26ef" class="graf graf--p graf-after--pre">(This isn’t great API design — ideally, we would decouple the backprop and weight update from the rest of the object, so the specific algorithm we use for updating weights isn’t tied to the layer itself. But that’s not the point, so we’ll stick with this design for the purposes of explaining how backpropagation works in a real-life scenario. Also: we’ll be using <a href="http://www.numpy.org/" data-href="http://www.numpy.org/" class="markup--anchor markup--p-anchor">numpy</a> throughout the implementation. It’s an awesome tool for mathematical operations in Python (especially tensor based ones), but we don’t have the time to get into how it works — if you want a good introduction, <a href="http://cs231n.github.io/python-numpy-tutorial/" data-href="http://cs231n.github.io/python-numpy-tutorial/" class="markup--anchor markup--p-anchor">here ya’ go</a>.)</p><p name="bdcd" id="bdcd" class="graf graf--p graf-after--p">We can start by implementing the weight initialization. As it turns out, how you initialize your weights is actually kind of a big deal for both network performance and convergence rates. Here’s how we’ll initialize our weights:</p><pre name="379d" id="379d" class="graf graf--pre graf-after--p">self.W = np.random.randn(self.size_in, self.size_out) * np.sqrt(2.0/self.size_in)</pre><p name="c4b1" id="c4b1" class="graf graf--p graf-after--pre">This initializes a weight matrix of the appropriate dimensions with random values sampled from a normal distribution. We then scale it rad(2/self.size_in), giving us a variance of 2/self.size_in (derivation <a href="http://arxiv-web3.library.cornell.edu/abs/1502.01852" data-href="http://arxiv-web3.library.cornell.edu/abs/1502.01852" class="markup--anchor markup--p-anchor">here</a>).</p><p name="7407" id="7407" class="graf graf--p graf-after--p">And that’s all we need for layer initialization! Let’s move on to implementing our first objective — feed-forward. This is actually pretty simple — a dot product of our input activations with the weight matrix, followed by our activation function, will give us the activations we need. The dot product part should make intuitive sense; if it doesn’t, you should sit down and try to work through it on a piece of paper. This is where the performance gains of grouping neurons into layers comes from: instead of keeping an individual weight vector for each neuron, and performing a series of vector dot products, we can just do a single matrix operation (which, thanks to the wonders of modern processors, is significantly faster). In fact, we can compute all of the activations from a layer in just two lines:</p><pre name="dd0f" id="dd0f" class="graf graf--pre graf-after--p"># Compute weighted sum for each neuron<br>self.out_act = np.dot(self.in_act, self.W)</pre><pre name="4eae" id="4eae" class="graf graf--pre graf-after--pre"># Activation function (any sum &lt; 0 is capped at 0)<br>self.out_act[self.out_act &lt; 0] = 0</pre><pre name="6796" id="6796" class="graf graf--pre graf-after--pre">return self.out_act</pre><p name="1674" id="1674" class="graf graf--p graf-after--pre">Simple enough. Let’s move on to backpropagation.</p><p name="3229" id="3229" class="graf graf--p graf-after--p">This one’s a bit more involved. First, we compute the derivative of the output w.r.t. the weights, then the derivative of the cost w.r.t. the output, followed by chain rule to get the derivative of the cost w.r.t. the weights.</p><p name="101b" id="101b" class="graf graf--p graf-after--p">Let’s start with the first part — the derivative of the output w.r.t. the weights. That should be simple enough; because you’re multiplying the weight by the corresponding input activation, the derivative will just be the corresponding input activation.</p><pre name="93aa" id="93aa" class="graf graf--pre graf-after--p">output_wrt_weights = np.ones(self.W.shape) * self.in_act[:, None]</pre><p name="5081" id="5081" class="graf graf--p graf-after--pre">Except, because we’re using the ReLU activation function, the weights have no effect if the corresponding output is &lt; 0 (because it gets capped anyway). This should take care of that hiccup:</p><pre name="dcc6" id="dcc6" class="graf graf--pre graf-after--p">output_wrt_weights[:, self.out_act &lt; 0] = 0</pre><p name="8476" id="8476" class="graf graf--p graf-after--pre">(More formally, you’re multiplying by the derivative of the activation function, which is 0 when the activation is &lt; 0 and 1 elsewhere.)</p><p name="1c81" id="1c81" class="graf graf--p graf-after--p">Let’s take a brief detour to talk about the <strong class="markup--strong markup--p-strong">out_grad</strong> parameter that our <strong class="markup--strong markup--p-strong">backward</strong> method gets. Let’s say we have a network with two layers: the first has <strong class="markup--strong markup--p-strong">m</strong> neurons, and the second has <strong class="markup--strong markup--p-strong">n</strong>. Each of the <strong class="markup--strong markup--p-strong">m </strong>neurons produces an activation, and each of the <strong class="markup--strong markup--p-strong">n</strong> neurons looks at each of the <strong class="markup--strong markup--p-strong">m </strong>activations. The <strong class="markup--strong markup--p-strong">out_grad </strong>parameter is an <strong class="markup--strong markup--p-strong">m </strong>x <strong class="markup--strong markup--p-strong">n </strong>matrix of how each <strong class="markup--strong markup--p-strong">m</strong> affects each of the <strong class="markup--strong markup--p-strong">n</strong> neurons it feeds into.</p><p name="1897" id="1897" class="graf graf--p graf-after--p">Now, we need the derivative of the cost w.r.t. each of the outputs — which is essentially the <strong class="markup--strong markup--p-strong">out_grad</strong> parameter we’re given! We just need to sum up each row of the matrix we’re given, as per the backpropagation formula.</p><pre name="8c3d" id="8c3d" class="graf graf--pre graf-after--p">cost_wrt_output = np.sum(np.atleast_2d(grad), axis=1)</pre><p name="6f19" id="6f19" class="graf graf--p graf-after--pre">Finally, we end up with something like this:</p><pre name="ae00" id="ae00" class="graf graf--pre graf-after--p">self.dW = cost_wrt_weights</pre><p name="1940" id="1940" class="graf graf--p graf-after--pre">Now, we need to compute the derivative of our inputs to pass along to the next layer. We can perform a similar chain rule — derivative of the output w.r.t. the inputs times the derivative of the cost w.r.t. the outputs.</p><pre name="a3ee" id="a3ee" class="graf graf--pre graf-after--p">output_wrt_inputs = self.W<br>output_wrt_inputs[:, self.out_act &lt; 0] = 0</pre><pre name="8547" id="8547" class="graf graf--pre graf-after--pre">cost_wrt_inputs = cost_wrt_output * output_wrt_inputs</pre><pre name="5d7b" id="5d7b" class="graf graf--pre graf-after--pre">return cost_wrt_inputs</pre><p name="c4f3" id="c4f3" class="graf graf--p graf-after--pre">And that’s it for the backpropagation step.</p><p name="6e9c" id="6e9c" class="graf graf--p graf-after--p">The final step is the weight update. Assuming we’re sticking with gradient descent for this example, this can be a simple one-liner:</p><pre name="c540" id="c540" class="graf graf--pre graf-after--p">self.W = self.W — self.dW * alpha</pre><p name="07ec" id="07ec" class="graf graf--p graf-after--pre">To actually train our network, we take one of our training samples and call <strong class="markup--strong markup--p-strong">forward</strong> on each layer consecutively, passing the output of the previous layer as the input of the following layer. We compute dJ, passing that as the <strong class="markup--strong markup--p-strong">out_grad</strong> parameter to the last layer’s <strong class="markup--strong markup--p-strong">backward</strong> method. We call <strong class="markup--strong markup--p-strong">backward</strong> on each of the layers in reverse order, this time passing the output of the further layer as <strong class="markup--strong markup--p-strong">out_grad </strong>to the previous layer. Finally, we call <strong class="markup--strong markup--p-strong">update </strong>on each of our layers and repeat.</p><p name="7ee1" id="7ee1" class="graf graf--p graf-after--p">There’s one last detail that we should include, which is the concept of a <strong class="markup--strong markup--p-strong">bias </strong>(akin to that of a constant term in any given equation). Notice that, with our current implementation, the activation of a neuron is determined solely based on the activations of the previous layer. There’s no bias term that can shift the activation up or down independent of the inputs. A bias term isn’t strictly necessary — in fact, if you train your network as-is, it would probably still work fine. But if you do need a bias term, the code stays almost the same — the only difference is that you need to add a column of 1s to the incoming activations, and update your weight matrix accordingly, so one of your weights gets treated as a bias term. The only other difference is that, when returning <strong class="markup--strong markup--p-strong">cost_wrt_inputs</strong>, you can cut out the first row — nobody cares about the gradients associated with the bias term because the previous layer has no say in the activation of the bias neuron.</p><p name="b1b8" id="b1b8" class="graf graf--p graf-after--p">Implementing backpropagation can be kind of tricky, so it’s often a good idea to check your implementation. You can do so by computing the gradient numerically (by literally perturbing the weight and calculating the difference in your cost function) and comparing it to your backpropagation-computed gradient. This <strong class="markup--strong markup--p-strong">gradient check</strong> doesn’t need to be run once you’ve verified your implementation, but it could save a lot of time tracking down potential problems with your network.</p><p name="feed" id="feed" class="graf graf--p graf-after--p">Nowadays, you often don’t even need to implement a neural network on your own, as libraries such as <a href="http://caffe.berkeleyvision.org/" data-href="http://caffe.berkeleyvision.org/" class="markup--anchor markup--p-anchor">Caffe</a>, <a href="http://torch.ch/" data-href="http://torch.ch/" class="markup--anchor markup--p-anchor">Torch</a>, or <a href="http://tensorflow.org/" data-href="http://tensorflow.org" class="markup--anchor markup--p-anchor">TensorFlow</a> will have implementations ready to go. That being said, it’s often a good idea to try implementing it on your own to get a better grasp of how everything works under the hood.</p><h4 name="c3a6" id="c3a6" class="graf graf--h4 graf-after--p">Learning More about Neural&nbsp;Networks</h4><p name="38e2" id="38e2" class="graf graf--p graf-after--h4">Intrigued? Looking to learn more about neural networks? Here are some great online classes to get you started:</p><p name="bf46" id="bf46" class="graf graf--p graf-after--p"><a href="http://cs231n.stanford.edu/" data-href="http://cs231n.stanford.edu" class="markup--anchor markup--p-anchor"><strong class="markup--strong markup--p-strong">Stanford’s CS231n</strong></a>. Although it’s technically about convolutional neural networks, the class provides an excellent introduction to and survey of neural networks in general. Class videos, notes, and assignments are all posted <a href="http://cs231n.stanford.edu/syllabus.html" data-href="http://cs231n.stanford.edu/syllabus.html" class="markup--anchor markup--p-anchor">here</a>, and if you have the patience for it I would strongly recommend walking through the assignments so you can really get to know what you’re learning.</p><p name="b3f7" id="b3f7" class="graf graf--p graf-after--p graf--last"><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" data-href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" class="markup--anchor markup--p-anchor"><strong class="markup--strong markup--p-strong">MIT 6.034</strong></a>. This class, taught by Prof. Patrick Henry Winston, explores many different algorithms and disciplines in Artificial Intelligence. There’s a great <a href="https://www.youtube.com/watch?v=q0pm3BrIUFo" data-href="https://www.youtube.com/watch?v=q0pm3BrIUFo" class="markup--anchor markup--p-anchor">lecture</a> on backprop that I actually used as a stepping stone to getting setup writing this article. I also learned genetic algorithms from Prof. Winston — he’s a great teacher!</p></div></div></section><section name="caa9" class="section section--body section--last"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner layoutSingleColumn"><p name="39ed" id="39ed" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf--leading graf--last"><span class="graf-dropCap">We</span> hope that, if you visited this article without knowing how the backpropagation algorithm works, you are reading this with an (at least rudimentary) mathematical or conceptual intuition of it. Writing and conveying such a complex algorithm to a supposed beginner has proven to be an extremely difficult task for us, but it’s helped us truly understand what we’ve been learning about. With greater knowledge in a fundamental area of machine learning, we are now excited to take a look at new, interesting algorithms and disciplines in the field. We are looking forward to continue documenting these endeavors <strong class="markup--strong markup--p-strong">together</strong>.</p></div></div></section></main><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><div class="tags tags--postTags tags--borderless"><a class="link u-baseColor--link" href="https://ayearofai.com/tagged/neural-networks?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Neural Networks</a><a class="link u-baseColor--link" href="https://ayearofai.com/tagged/machine-learning?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Machine Learning</a><a class="link u-baseColor--link" href="https://ayearofai.com/tagged/algorithms?source=post" data-action-source="post" data-collection-slug="a-year-of-artificial-intelligence">Algorithms</a></div></div></div></div><div class="row js-postActionsFooter"><div class="postActions col u-size12of12"><div class="u-floatLeft buttonSet buttonSet--withLabels"><div class="buttonSet-inner"><div class="js-actionRecommend" data-post-id="abf4609d4f9d" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_actions_footer"><button class="button button--primary button--large button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton" title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/abf4609d4f9d" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.215 23.716c-.348.288-.984.826-1.376 1.158a.526.526 0 0 1-.68 0c-.36-.307-.92-.78-1.22-1.03C9.22 20.734 3 15.527 3 10.734 3 7.02 5.916 4 9.5 4c1.948 0 3.77.898 5 2.434C15.73 4.898 17.552 4 19.5 4c3.584 0 6.5 3.02 6.5 6.734 0 4.9-6.125 9.96-9.785 12.982zM19.5 5.2c-1.774 0-3.423.923-4.41 2.468a.699.699 0 0 1-.59.323.706.706 0 0 1-.59-.32c-.988-1.54-2.637-2.47-4.41-2.47-2.922 0-5.3 2.49-5.3 5.54 0 4.23 6.19 9.41 9.517 12.19.217.18.566.48.783.66l.952-.79c3.496-2.88 9.348-7.72 9.348-12.05 0-3.05-2.378-5.53-5.3-5.53z"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.5 4c-1.948 0-3.77.898-5 2.434C13.27 4.898 11.448 4 9.5 4 5.916 4 3 7.02 3 10.734c0 4.793 6.227 10 9.95 13.11.296.25.853.723 1.212 1.03.196.166.48.166.677 0 .39-.332 1.02-.87 1.37-1.158 3.66-3.022 9.79-8.08 9.79-12.982C26 7.02 23.08 4 19.5 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="show-recommends" data-action-value="abf4609d4f9d">34</button></div></div><div class="buttonSet-inner"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="scroll-to-responses">1</button></div></div><div class="u-floatRight buttonSet buttonSet--narrow"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button></div></div></div></div><div class="js-postPromotionWrapper postPromotionWrapper" data-location-id="footer_above_post_attribution"></div><div class="container u-maxWidth740 js-postAttributionFooterContainer"><div class="u-margin20"></div><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="card card-user"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="footer_card_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell "><a class="link avatar u-baseColor--link" href="https://ayearofai.com/@mckapur?source=footer_card" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" data-action-source="footer_card" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1f_drXOgflIo8yaxCbSDOmQ_003.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of Rohan Kapur"></a></div><div class="card-content"><h3 class="card-name"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ayearofai.com/@mckapur" property="cc:attributionName" title="Go to the profile of Rohan Kapur" aria-label="Go to the profile of Rohan Kapur" rel="author cc:attributionUrl" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a></h3><p class="card-description">rohankapur.com</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30 u-xs-marginBottom20"><li class="card card-collection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton" data-action="sign-in-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/a-year-of-artificial-intelligence" data-collection-id="bb87da25612c"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><div class="u-tableCell "><a class="link avatar avatar--roundedRectangle u-baseColor--link" href="https://ayearofai.com/?source=footer_card" title="Go to A year of Artificial Intelligence" aria-label="Go to A year of Artificial Intelligence" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1NZsNSuNxe_O2YW1ybboOvA.jpeg" class="avatar-image u-size60x60" alt="A year of Artificial Intelligence"></a></div></div><div class="card-content"><h3 class="card-name"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://ayearofai.com/?source=footer_card" rel="collection" data-action-source="footer_card" data-collection-slug="a-year-of-artificial-intelligence">A year of Artificial Intelligence</a></h3><p class="card-description">2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postBundleWrapper postBundleWrapper"></div><div class="js-postFooterPlacements"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="u-marginAuto u-maxWidth1000 u-clearfix u-paddingBottom40 u-paddingTop30"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div data-scroll="native" class="u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="1a1b16f65dda" data-source="placement_card_footer_grid---------0-45" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda?source=placement_card_footer_grid---------0-45" data-action-source="placement_card_footer_grid---------0-45"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-sizeFullWidth u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*Nej6qkNKxLp1tCqiftEj0A.jpeg&quot;);background-position: 50% 50%;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-flex1 u-baseColor--link" href="https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda?source=placement_card_footer_grid---------0-45" data-action-source="placement_card_footer_grid---------0-45"><div class="postMetaInline u-marginBottom7">More on Machine Learning from A year of Artificial Intelligence</div><div class="u-textColorDarkest u-contentSansBold u-fontSizeLarge u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Rohan #3: Deriving the Normal Equation using matrix calculus</div></a><div class="u-paddingTop10 u-flex0 u-flexCenter"><div class="u-flex1 u-noWrapWithEllipsis"><div class="postMetaInline-avatar"><a class="link avatar u-baseColor--link" href="https://ayearofai.com/@mckapur" data-action="show-user-card" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1f_drXOgflIo8yaxCbSDOmQ.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Rohan Kapur"></a></div><div class="postMetaInline-authorLockup"><a class="link link link--darken link--darker u-baseColor--link" href="https://ayearofai.com/@mckapur?source=placement_card_footer_grid---------0-45" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-45" data-action-type="hover" data-user-id="cb55958ea3bb" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Rohan Kapur</a><div class="u-fontSizeSmallest js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal">11 min read</span></div></div></div><div class="u-flex0 u-flexCenter"><div class="u-inlineBlock"><div class="js-actionRecommend" data-post-id="1a1b16f65dda" data-is-label-padded="true" data-source="placement_card_footer_grid---------0-45"><button class="button button--primary button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton" title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/1a1b16f65dda" data-action-source="placement_card_footer_grid---------0-45"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12zM8.428 4.866c-2.414 0-4.378 2.05-4.378 4.568 0 3.475 5.057 7.704 7.774 9.975.243.2.47.39.676.56.245-.21.52-.43.813-.68 2.856-2.36 7.637-6.31 7.637-9.87 0-2.52-1.964-4.57-4.377-4.57-1.466 0-2.828.76-3.644 2.04-.1.14-.26.23-.43.23-.18 0-.34-.09-.43-.24-.82-1.27-2.18-2.03-3.65-2.03z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="1a1b16f65dda">12</button></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-positionRelative u-marginRight10 u-marginLeft12"></div><div class="u-inlineBlock"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-in-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/1a1b16f65dda"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="26" viewBox="0 0 25 26"><path d="M19 7c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 17.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V7z" fill-rule="evenodd"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div data-scroll="native" class="u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="ae6e69ff4cf0" data-source="placement_card_footer_grid---------1-45" data-tracking-context="placement"><div class="u-padding15 u-borderBox u-flexColumn u-sizeFull"><a class="link link--noUnderline u-flex1 u-flexColumn u-baseColor--link" href="https://ayearofai.com/lenny-1-robots-reinforcement-learning-ae6e69ff4cf0?source=placement_card_footer_grid---------1-45" data-action-source="placement_card_footer_grid---------1-45"><div class="postMetaInline u-marginBottom7">More on Algorithms from A year of Artificial Intelligence</div><div class="u-textColorDarkest u-contentSansBold u-fontSizeLarge u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Lenny #1: Robots + Reinforcement Learning</div><div class="u-textColorNormal u-fontSize22 u-contentSansThin u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-marginTop7 u-paddingBottom2">This is my first post for A Year of AI, a project that Rohan Kapur and I will be undertaking over the next year to learn more about the…</div></a><div class="u-paddingTop10 u-flex0 u-flexCenter"><div class="u-flex1 u-noWrapWithEllipsis"><div class="postMetaInline-avatar"><a class="link avatar u-baseColor--link" href="https://ayearofai.com/@lennykhazan" data-action="show-user-card" data-action-type="hover" data-user-id="de8e2540b759" data-collection-slug="a-year-of-artificial-intelligence" dir="auto"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1r4uTIoF2zTSYRtc7p_ukww.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Lenny Khazan"></a></div><div class="postMetaInline-authorLockup"><a class="link link link--darken link--darker u-baseColor--link" href="https://ayearofai.com/@lennykhazan?source=placement_card_footer_grid---------1-45" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-45" data-action-type="hover" data-user-id="de8e2540b759" data-collection-slug="a-year-of-artificial-intelligence" dir="auto">Lenny Khazan</a><div class="u-fontSizeSmallest js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal">5 min read</span></div></div></div><div class="u-flex0 u-flexCenter"><div class="u-inlineBlock"><div class="js-actionRecommend" data-post-id="ae6e69ff4cf0" data-is-label-padded="true" data-source="placement_card_footer_grid---------1-45"><button class="button button--primary button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton" title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/ae6e69ff4cf0" data-action-source="placement_card_footer_grid---------1-45"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12zM8.428 4.866c-2.414 0-4.378 2.05-4.378 4.568 0 3.475 5.057 7.704 7.774 9.975.243.2.47.39.676.56.245-.21.52-.43.813-.68 2.856-2.36 7.637-6.31 7.637-9.87 0-2.52-1.964-4.57-4.377-4.57-1.466 0-2.828.76-3.644 2.04-.1.14-.26.23-.43.23-.18 0-.34-.09-.43-.24-.82-1.27-2.18-2.03-3.65-2.03z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="ae6e69ff4cf0">4</button></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-positionRelative u-marginRight10 u-marginLeft12"></div><div class="u-inlineBlock"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-in-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/ae6e69ff4cf0"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="26" viewBox="0 0 25 26"><path d="M19 7c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 17.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V7z" fill-rule="evenodd"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div data-scroll="native" class="u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="86f11fcee54" data-source="placement_card_footer_grid---------2-43" data-tracking-context="placement"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/@vzkuma/4-steps-for-learning-deep-learning-86f11fcee54?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-sizeFullWidth u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/400/120/1*LPLTl4QRNOgNU6xy2cT-EA.jpeg&quot;);background-position: 50% 50%;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-flex1 u-baseColor--link" href="https://medium.com/@vzkuma/4-steps-for-learning-deep-learning-86f11fcee54?source=placement_card_footer_grid---------2-43" data-action-source="placement_card_footer_grid---------2-43"><div class="postMetaInline u-marginBottom7">Also tagged Neural Networks</div><div class="u-textColorDarkest u-contentSansBold u-fontSizeLarge u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">4 Steps for Learning Deep Learning</div></a><div class="u-paddingTop10 u-flex0 u-flexCenter"><div class="u-flex1 u-noWrapWithEllipsis"><div class="postMetaInline-avatar"><a class="link avatar u-baseColor--link" href="https://medium.com/@vzkuma" data-action="show-user-card" data-action-type="hover" data-user-id="6fab70caac98" dir="auto"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1Z8GO7gponcy9UNhT-viO5w.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Vivek Kumar"></a></div><div class="postMetaInline-authorLockup"><a class="link link link--darken link--darker u-baseColor--link" href="https://medium.com/@vzkuma?source=placement_card_footer_grid---------2-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-43" data-action-type="hover" data-user-id="6fab70caac98" dir="auto">Vivek Kumar</a><div class="u-fontSizeSmallest js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal">5 min read</span></div></div></div><div class="u-flex0 u-flexCenter"><div class="u-inlineBlock"><div class="js-actionRecommend" data-post-id="86f11fcee54" data-is-label-padded="true" data-source="placement_card_footer_grid---------2-43"><button class="button button--primary button--chromeless is-touchIconFadeInPulse u-accentColor--buttonNormal button--withIcon button--withSvgIcon u-accentColor--iconLight js-actionRecommendButton" title="Recommend to share this article with your followers and let the author know you liked it" aria-label="Recommend to share this article with your followers and let the author know you liked it" data-action="sign-in-prompt" data-sign-in-action="upvote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/86f11fcee54" data-action-source="placement_card_footer_grid---------2-43"><span class="button-defaultState"><span class="svgIcon svgIcon--heart svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12zM8.428 4.866c-2.414 0-4.378 2.05-4.378 4.568 0 3.475 5.057 7.704 7.774 9.975.243.2.47.39.676.56.245-.21.52-.43.813-.68 2.856-2.36 7.637-6.31 7.637-9.87 0-2.52-1.964-4.57-4.377-4.57-1.466 0-2.828.76-3.644 2.04-.1.14-.26.23-.43.23-.18 0-.34-.09-.43-.24-.82-1.27-2.18-2.03-3.65-2.03z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--heartFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M12.5 21a.492.492 0 0 1-.327-.122c-.278-.24-.61-.517-.978-.826-2.99-2.5-7.995-6.684-7.995-10.565C3.2 6.462 5.578 4 8.5 4c1.55 0 3 .695 4 1.89a5.21 5.21 0 0 1 4-1.89c2.923 0 5.3 2.462 5.3 5.487 0 3.97-4.923 8.035-7.865 10.464-.42.35-.798.66-1.108.93a.503.503 0 0 1-.327.12z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="86f11fcee54">121</button></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-positionRelative u-marginRight10 u-marginLeft12"></div><div class="u-inlineBlock"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-in-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/86f11fcee54"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="26" viewBox="0 0 25 26"><path d="M19 7c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 17.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V7z" fill-rule="evenodd"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div data-action-scope="_actionscope_5" class="responsesWrapper supplementalPostContent js-responsesWrapper"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container u-maxWidth640 responsesStream-title u-paddingTop15"><div class="row"><div class="col u-size12of12"><div class="heading u-clearfix"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></div></div></div></div><div class="container u-maxWidth640 u-marginBottom30 responsesStream-editor cardChromeless js-responsesStreamEditor"><div class="row"><div class="col u-size12of12"><div class="responses-loggedOutPrompt js-responsesLoggedOutPrompt"><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--withIconAndLabel button--loggedOutPrompt" data-action="sign-in-prompt" data-redirect="https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#--respond" data-action-source="logged_out_response_prompt"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span><span class="button-label  js-buttonLabel">Write a response…</span></button></div></div></div></div><div class="container u-maxWidth640 u-marginBottom30 u-borderTopLighter u-hide js-noOtherResponses"><div class="row"><div class="col u-size12of12"><div class="u-paddingTop20 u-paddingBottom25"><div class="u-inlineBlock u-verticalAlignTop u-paddingRight10"><div class="avatar avatar--inline"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/default-avatar.png" class="avatar-image u-size36x36 u-xs-size32x32" alt="Unknown user"></div></div><div class="u-inlineBlock"><div class="u-lineHeight36 u-textColorNormal u-contentSerifRegular">No responses yet</div></div></div></div></div></div><div class="responsesStream js-responsesStream"></div><div class="container u-maxWidth640 js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-sizeFullWidth" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-readNext"></div><div class="supplementalPostContent js-heroPromo"></div></footer></article><div class="promoCardWrapper u-positionFixed js-promoCardWrapper"><div class="promoCard u-padding20 u-backgroundWhite u-textColorDarker"><button class="button button--close button--chromeless u-baseColor--buttonNormal" data-action="popup-dismiss">×</button><div class="u-uiTextRegular u-paddingBottom20 u-paddingRight30 u-fontSizeSmall u-lineHeightTight u-marginBottom20 u-borderBottomLightest">Don’t miss Rohan Kapur’s next story</div><div class="u-floatRight"><span class="followState js-followState buttonSet-inner" data-user-id="cb55958ea3bb"><button class="button u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="sign-in-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-action-source="user_follow_popup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="sign-in-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/cb55958ea3bb" data-action-source="user_follow_popup_follow"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="avatar"><img src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/1f_drXOgflIo8yaxCbSDOmQ_002.jpeg" class="avatar-image avatar-image--smaller" alt="Rohan Kapur"><span class="avatar-text">Rohan Kapur</span></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #049B80 !important;}
.u-accentColor--borderNormal {border-color: #049B80 !important;}
.u-accentColor--borderDark {border-color: #20846E !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #049B80 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #049B80 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #20846E !important;}
.u-accentColor--textNormal {color: #20846E !important;}
.u-accentColor--hoverTextNormal:hover {color: #20846E !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #247865 !important;}
.u-accentColor--textDark {color: #247865 !important;}
.u-accentColor--backgroundLight {background-color: #049B80 !important;}
.u-accentColor--backgroundNormal {background-color: #049B80 !important;}
.u-accentColor--backgroundDark {background-color: #20846E !important;}
.u-accentColor--buttonDark {border-color: #20846E !important; color: #247865 !important;}
.u-accentColor--buttonDark:hover {border-color: #247865 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #20846E !important; fill: #20846E !important;}
.u-accentColor--buttonNormal {border-color: #049B80 !important; color: #20846E !important;}
.u-accentColor--buttonNormal:hover {border-color: #20846E !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #049B80 !important; fill: #049B80 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #20846E !important; border-color: #20846E !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled,.u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #049B80 !important; border-color: #049B80 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #20846E !important;}.u-tintBgColor {background-color: rgba(22, 160, 133, 1) !important;}.u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(22, 160, 133, 1) 0%, rgba(22, 160, 133, 0) 100%) !important;}.u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(22, 160, 133, 0) 0%, rgba(22, 160, 133, 1) 100%) !important;}
.u-tintSpectrum .u-baseColor--borderLight {border-color: #9AD5C3 !important;}
.u-tintSpectrum .u-baseColor--borderNormal {border-color: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--borderDark {border-color: #E1FBF1 !important;}
.u-tintSpectrum .u-baseColor--iconLight .svgIcon,.u-tintSpectrum .u-baseColor--iconLight.svgIcon {fill: #9AD5C3 !important;}
.u-tintSpectrum .u-baseColor--iconNormal .svgIcon,.u-tintSpectrum .u-baseColor--iconNormal.svgIcon {fill: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--iconDark .svgIcon,.u-tintSpectrum .u-baseColor--iconDark.svgIcon {fill: #E1FBF1 !important;}
.u-tintSpectrum .u-baseColor--textNormal {color: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--textDark {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--backgroundLight {background-color: #9AD5C3 !important;}
.u-tintSpectrum .u-baseColor--backgroundNormal {background-color: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--backgroundDark {background-color: #E1FBF1 !important;}
.u-tintSpectrum .u-baseColor--buttonDark {border-color: #E1FBF1 !important; color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--buttonDark:hover {border-color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--buttonDark .icon:before,.u-tintSpectrum .u-baseColor--buttonDark .svgIcon {color: #E1FBF1 !important; fill: #E1FBF1 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal {border-color: #BEE9DB !important; color: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--buttonNormal:hover {border-color: #E1FBF1 !important;}
.u-tintSpectrum .u-baseColor--buttonNormal .icon:before,.u-tintSpectrum .u-baseColor--buttonNormal .svgIcon {color: #BEE9DB !important; fill: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--buttonDark.button--filled,.u-tintSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: #E1FBF1 !important; border-color: #E1FBF1 !important; color: rgba(22, 160, 133, 1) !important; fill: rgba(22, 160, 133, 1) !important;}
.u-tintSpectrum .u-baseColor--buttonNormal.button--filled,.u-tintSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: #BEE9DB !important; border-color: #BEE9DB !important; color: rgba(22, 160, 133, 1) !important; fill: rgba(22, 160, 133, 1) !important;}
.u-tintSpectrum .u-baseColor--link {color: #BEE9DB !important;}
.u-tintSpectrum .u-baseColor--link.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--darken:active {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--link.link--dark {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:active {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--link.link--darker {color: #F1FFFD !important;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: #9AD5C3;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: #9AD5C3;}
.u-tintSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: #9AD5C3;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: #5BB69F !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: #72C1AB !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: #9AD5C3 !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--borderLight {border-color: #9AD5C3 !important;}
.u-tintSpectrum .u-accentColor--borderNormal {border-color: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--borderDark {border-color: #E1FBF1 !important;}
.u-tintSpectrum .u-accentColor--iconLight .svgIcon,.u-tintSpectrum .u-accentColor--iconLight.svgIcon {fill: #9AD5C3 !important;}
.u-tintSpectrum .u-accentColor--iconNormal .svgIcon,.u-tintSpectrum .u-accentColor--iconNormal.svgIcon {fill: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--iconDark .svgIcon,.u-tintSpectrum .u-accentColor--iconDark.svgIcon {fill: #E1FBF1 !important;}
.u-tintSpectrum .u-accentColor--textNormal {color: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--hoverTextNormal:hover {color: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #F1FFFD !important;}
.u-tintSpectrum .u-accentColor--textDark {color: #F1FFFD !important;}
.u-tintSpectrum .u-accentColor--backgroundLight {background-color: #9AD5C3 !important;}
.u-tintSpectrum .u-accentColor--backgroundNormal {background-color: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--backgroundDark {background-color: #E1FBF1 !important;}
.u-tintSpectrum .u-accentColor--buttonDark {border-color: #E1FBF1 !important; color: #F1FFFD !important;}
.u-tintSpectrum .u-accentColor--buttonDark:hover {border-color: #F1FFFD !important;}
.u-tintSpectrum .u-accentColor--buttonDark .icon:before,.u-tintSpectrum .u-accentColor--buttonDark .svgIcon{color: #E1FBF1 !important; fill: #E1FBF1 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal {border-color: #BEE9DB !important; color: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:hover {border-color: #E1FBF1 !important;}
.u-tintSpectrum .u-accentColor--buttonNormal .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal .svgIcon{color: #BEE9DB !important; fill: #BEE9DB !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(22, 160, 133, 1) !important; fill: rgba(22, 160, 133, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonDark.button--filled,.u-tintSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-tintSpectrum .u-accentColor--fillWhenActive.is-active {background-color: #E1FBF1 !important; border-color: #E1FBF1 !important; color: rgba(22, 160, 133, 1) !important; fill: rgba(22, 160, 133, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled,.u-tintSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: #BEE9DB !important; border-color: #BEE9DB !important; color: rgba(22, 160, 133, 1) !important; fill: rgba(22, 160, 133, 1) !important;}
.u-tintSpectrum .postArticle.is-withAccentColors .markup--user,.u-tintSpectrum .postArticle.is-withAccentColors .markup--query {color: #BEE9DB !important;}
.u-accentColor--highlightFaint {background-color: rgba(220, 249, 238, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(159, 244, 219, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(220, 249, 238, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(220, 249, 238, 1), rgba(220, 249, 238, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(188, 246, 227, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(188, 246, 227, 1), rgba(188, 246, 227, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(159, 244, 219, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(159, 244, 219, 1), rgba(159, 244, 219, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(159, 244, 219, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(159, 244, 219, 1), rgba(159, 244, 219, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(159, 244, 219, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(159, 244, 219, 1), rgba(159, 244, 219, 1));}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(255, 255, 255, 0.4) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(255, 255, 255, 0.4980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled,.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(0, 0, 0, 0.4) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled,.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style><div data-action-scope="_actionscope_3" class="highlightMenu"><div class="highlightMenu-inner"><div class="buttonSet buttonSet--highlightMenu"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="sign-in-prompt" data-sign-in-action="quote" data-requires-token="true" data-redirect-type="quote" data-action-source="quote_menu"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu " data-action="sign-in-prompt" data-sign-in-action="quote-respond" data-action-source="quote_menu"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu " data-action="twitter" data-action-source="quote_menu"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></button><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu " data-action="sign-in-prompt" data-sign-in-action="highlight" data-action-source="quote_menu"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://ayearofai.com","bestOfFirstSlug":"may-2013","bestOfLatestSlug":"april-2016","buildLabel":"25114-fa06199","currentUser":{"userId":"lo_91b04744946","isVerified":false,"subscriberEmail":""},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"language":"en-us","loadingPlaceholderImg":"https://cdn-static-1.medium.com/_/fp/img/media-loading-placeholder.b31hiO4ynbDLRrXWEFF4aQ.png","mediumTwitterScreenName":"medium","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/js/main-base.bundle.3vy-OJ4rGgbEizZK3anPEA.js","notes":"https://cdn-static-1.medium.com/_/fp/js/main-notes.bundle.EwQbf1uWgXC72FJFBKcReQ.js","posters":"https://cdn-static-1.medium.com/_/fp/js/main-posters.bundle.p_2TgPbSAv6bSKWFYoJPtQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/js/main-common-async.bundle.gKjc5qLmacs78wW9-Mk8xw.js","stats":"https://cdn-static-1.medium.com/_/fp/js/main-stats.bundle.wC_6bQxU1Ed24HN08xLlDw.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/js/main-misc-screens.bundle.OxjhULRSrEWk8m-iPPI_RA.js"},"postColumnWidth":700,"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1474420757145:9e9d86ab286b","useragent":{"browser":"firefox","family":"firefox","os":"","version":48,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isFacebook":false,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"isGoogle":false,"isApple":false,"supportsPromotedPosts":true,"isBing":false,"isYahoo":false,"isTwitter":false,"isBaidu":false,"isSlack":false,"isSimplePie":false,"isYandex":false,"isBot":false,"isSocialMediaBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","enable_lightstep_web_client":true,"signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","enable_smyte_integration":true,"enable_service_worker":true,"android_rating_prompt_recommend_threshold":5,"google_sign_in_android":true,"enable_onboarding":true,"custom_miro_url":"https://cdn-images-1.medium.com","ios_custom_miro_url":"https://cdn-images-1.medium.com","reengagement_notification_duration":3,"enable_user_social_count_healing":true,"enable_related_reads_webui":true,"enable_post_bundles_strategy_tag_based":true,"enable_hopper_for_tag_based_post_bundles":true,"enable_post_bundles_strategy_author_based":true,"enable_post_bundles_strategy_collection_based":true,"show_author_writes_about":true,"top_stories_experiment_source":"topStories02","enable_next_tick_update":true,"enable_adsnative_integration":true,"browsable_stream_config_bucket":"curated","ios_small_post_preview_truncation_length":5.5,"ios_large_post_preview_truncation_length":5.5,"disable_ios_catalog_badging":true,"enable_ranked_feed_survey_promo":true,"enable_textshots_v2":true,"enable_textshots_v2_web_ui":true,"enable_dark_sign_in_modals":true,"enable_ios_personalization_promo":true,"enable_personalization_station_web":true,"add_top_stories_in_digest":true,"digest_experiment":"control","enable_digest_snapshot":true,"enable_prepublish_share_settings":true,"enable_direct_auth_connect":true,"enable_prepublish_twitter_connect":true,"enable_sponsored_post_labelling":true,"enable_collection_post_nav_item":true,"enable_logged_in_follow_on_collection_post":true,"enable_placements":true,"social_recs_weight_factor":1,"digest_total_post_count":30,"digest_posts_count_published_by_followed_users_and_collections":16,"digest_posts_count_recommended_by_users":5,"digest_posts_count_recommended_by_staff":5,"digest_posts_count_per_tag":2,"enable_prepublish_facebook_share":true,"enable_prepublish_facebook_connect":true,"promoted_story_placement_locations":"POST_PAGE_FOOTER","rollup_responses_in_activity_feed":true,"enable_stats_response_filtering":true,"enable_ios_feature_education":true,"enable_email_curse_words_filtering":true,"enable_related_reads_curse_words_filtering":true,"enable_normalized_email_check":true,"enable_search_collection_by_tag_recency_filter":true,"search_collection_by_tag_filter_min_votes":10,"enable_stream_ttr_on_post_page":true,"scroll_to_next_stack_size":5,"slow_retries_on_throttling":true,"enable_user_blocks_bloom_filter":true,"show_read_history":true,"enable_sentiment_analysis":true,"honeypot_strategy":"entity"},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","teamName":"Team Medium","fp":{"/img/apple-touch-icon-ipad-retina.png":"https://cdn-static-1.medium.com/_/fp/img/apple-touch-icon-ipad-retina.Akq8aSfZqfW1fceMxKqBZA.png","/img/apple-touch-icon-iphone-retina.png":"https://cdn-static-1.medium.com/_/fp/img/apple-touch-icon-iphone-retina.c211N_zSkSXPQk-ggPi4mQ.png","/img/apple-touch-icon-ipad.png":"https://cdn-static-1.medium.com/_/fp/img/apple-touch-icon-ipad.LSTr_8Uf-3hSd7eDjoW_8g.png","/img/apple-touch-icon.png":"https://cdn-static-1.medium.com/_/fp/img/apple-touch-icon.JWwtHOsKxVkBzoR3FSccjw.png","/img/default-avatar.png":"https://cdn-static-1.medium.com/_/fp/img/default-avatar.dmbNkD5D-u45r44go_cf0g.png","/img/default-preview-image.png":"https://cdn-static-1.medium.com/_/fp/img/default-preview-image.IsBK38jFAJBlWifMLO4z9g.png","/img/default-preview-image-v2.png":"https://cdn-static-1.medium.com/_/fp/img/default-preview-image-v2.MXL-j6S8fTEd8UFP_foEEw.png","/img/email/app_store_badge@2x.png":"https://cdn-static-1.medium.com/_/fp/img/email/app_store_badge@2x.8bDQGNMm-Xs7Hz6WA2XquQ.png","/img/email/app-devices@2x.png":"https://cdn-static-1.medium.com/_/fp/img/email/app-devices@2x.6hgpI423F62SKyT8Lo6dzA.png","/img/email/check1.png":"https://cdn-static-1.medium.com/_/fp/img/email/check1.0DM77li7vZhq5o2V9cVYLQ.png","/img/email/check2.png":"https://cdn-static-1.medium.com/_/fp/img/email/check2.GLlNusQmn1hwo9WDN-gE1w.png","/img/email/check3.png":"https://cdn-static-1.medium.com/_/fp/img/email/check3.7VxOUVMXAVbHRRnzMrJ_5A.png","/img/email/email-logo.png":"https://cdn-static-1.medium.com/_/fp/img/email/email-logo.x91rxfZYzIT9OJ5-ySD30A.png","/img/email/email-wordmark.png":"https://cdn-static-1.medium.com/_/fp/img/email/email-wordmark.gDTi-pejtOeN6Ux6a0MNFQ.png","/img/email/fb_logo.png":"https://cdn-static-1.medium.com/_/fp/img/email/fb_logo.Q0M98YwNTu77gLWTK6-RyQ.png","/img/email/google_play_badge@2x.png":"https://cdn-static-1.medium.com/_/fp/img/email/google_play_badge@2x.iWEWlt5_Qj20rr79-IGEBQ.png","/img/email/heart.png":"https://cdn-static-1.medium.com/_/fp/img/email/heart._hlyuYQiuuTWrRmDnDphJA.png","/img/email/heart@2x.png":"https://cdn-static-1.medium.com/_/fp/img/email/heart@2x.qvCTX1XHNpntqxt01oypQw.png","/img/email/heart1.png":"https://cdn-static-1.medium.com/_/fp/img/email/heart1.rnGEmSwcGUhztl_zSU7l6Q.png","/img/email/heart2.png":"https://cdn-static-1.medium.com/_/fp/img/email/heart2.HBiLu3koIYsKjjKroohgbA.png","/img/email/heart3.png":"https://cdn-static-1.medium.com/_/fp/img/email/heart3.AIJBOHw11HuhdClVJNtmtg.png","/img/email/large.png":"https://cdn-static-1.medium.com/_/fp/img/email/large.4EIhZbIk5sgqYSoB4YPmUA.png","/img/email/response.png":"https://cdn-static-1.medium.com/_/fp/img/email/response.5ZuN24N5XqS7ofHKg1lKXw.png","/img/email/response@2x.png":"https://cdn-static-1.medium.com/_/fp/img/email/response@2x.xzKQM4FzSurAVkghLQQWkA.png","/img/email/trophy.png":"https://cdn-static-1.medium.com/_/fp/img/email/trophy.YXqqMnNASKQDXZ46YdTWFA.png","/img/email/twitter_logo.png":"https://cdn-static-1.medium.com/_/fp/img/email/twitter_logo.Pz4a3o9WMU5QioxLKcyFhQ.png","/img/email/unlisted.png":"https://cdn-static-1.medium.com/_/fp/img/email/unlisted.ikh8R2LElOz_1YM8A2Db4g.png","/img/email/welcome-heart.png":"https://cdn-static-1.medium.com/_/fp/img/email/welcome-heart.6BRCOGcwGeOCBTql8pbq2g.png","/img/email/welcome-response.png":"https://cdn-static-1.medium.com/_/fp/img/email/welcome-response.iWVQLjiUG5pyQrPyGiYEpw.png","/img/email/welcome-write.png":"https://cdn-static-1.medium.com/_/fp/img/email/welcome-write.afdGsuE6YDk3HkaIASRl0w.png","/img/help/add-media-start.gif":"https://cdn-static-1.medium.com/_/fp/img/help/add-media-start.GXmqQ2Svt1WfZGIZSM93tg.gif","/img/help/add-media.gif":"https://cdn-static-1.medium.com/_/fp/img/help/add-media.SZH2LBmkwVExuhozFfVvYg.gif","/img/help/embed-start.gif":"https://cdn-static-1.medium.com/_/fp/img/help/embed-start.EBJ2PcWFJuYopsQV4wwklA.gif","/img/help/embed.gif":"https://cdn-static-1.medium.com/_/fp/img/help/embed.1BM0Di9vd91Kv8fLioJabw.gif","/img/help/text-highlight-start.gif":"https://cdn-static-1.medium.com/_/fp/img/help/text-highlight-start.AvTbakaFuUCd05YoMECoMQ.gif","/img/help/text-highlight.gif":"https://cdn-static-1.medium.com/_/fp/img/help/text-highlight.XVChoKYZ1-s3gJgHm9-7Yg.gif","/img/help/2.0-add-media.gif":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-add-media.LhrJcTFODoqjnPPuHNEInA.gif","/img/help/2.0-add-media-start.png":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-add-media-start.PTVdm36977cSfaYOpMNTNQ.png","/img/help/2.0-highlight-menu.gif":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-highlight-menu.eqlvF_kPTaDVcWZV4oPjVA.gif","/img/help/2.0-highlight-menu-start.png":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-highlight-menu-start.bkThTm0MV-KahZWttjBlJQ.png","/img/help/2.0-embed.gif":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-embed.2g5jIXYtWjsRFa9D-hBhMQ.gif","/img/help/2.0-embed-start.png":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-embed-start.LGy_FmixvcoEVIDKjg1jwA.png","/img/help/2.0-mention.gif":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-mention.H9pEv0eHAvkGUVqdrHxH5g.gif","/img/help/2.0-mention-start.png":"https://cdn-static-1.medium.com/_/fp/img/help/2.0-mention-start.ShYTOfy6w3VDU9PKFfOEyw.png","/img/help/highlight-tips-1.gif":"https://cdn-static-1.medium.com/_/fp/img/help/highlight-tips-1.dEp02HXMKtAeKxsV5MWqGg.gif","/img/help/highlight-tips-2.gif":"https://cdn-static-1.medium.com/_/fp/img/help/highlight-tips-2.SyXCciYXeNpx4-GUAuh0XQ.gif","/img/import/highlight-menu.png":"https://cdn-static-1.medium.com/_/fp/img/import/highlight-menu.kzoaVM8mJJ-Hu9m9uo3Omg.png","/img/import/image-highlight-menu.png":"https://cdn-static-1.medium.com/_/fp/img/import/image-highlight-menu.q43-H2dl0JvBS_5znQCW8A.png","/img/import/publish-metabar.png":"https://cdn-static-1.medium.com/_/fp/img/import/publish-metabar.YbEX1a2Pu0rAR_LuKeg8JA.png","/img/payments/amex.png":"https://cdn-static-1.medium.com/_/fp/img/payments/amex.5EPSpIzX7GdCzJlp6ScaDw.png","/img/payments/diners.png":"https://cdn-static-1.medium.com/_/fp/img/payments/diners.NGD5bmkc-37YhAWWR1kzZQ.png","/img/payments/discover.png":"https://cdn-static-1.medium.com/_/fp/img/payments/discover.EZDA3I2LqtScyoBjwnpyAA.png","/img/payments/jcb.png":"https://cdn-static-1.medium.com/_/fp/img/payments/jcb.r4YW_7gdVvO70Y1uobs9zw.png","/img/payments/mastercard.png":"https://cdn-static-1.medium.com/_/fp/img/payments/mastercard.pkLRUgj9PI_snp1LBC8FYQ.png","/img/payments/placeholder.png":"https://cdn-static-1.medium.com/_/fp/img/payments/placeholder.UD4yFjC9YHrS0yCBRmRwvQ.png","/img/payments/visa.png":"https://cdn-static-1.medium.com/_/fp/img/payments/visa.0d40297wdAUwEkxSXQjBoQ.png","/img/payments/credit-card-sprite.png":"https://cdn-static-1.medium.com/_/fp/img/payments/credit-card-sprite.aNIJTodZTkf86-MUkQX7Xw.png","/img/signup/signup_education.png":"https://cdn-static-1.medium.com/_/fp/img/signup/signup_education.U40idKqxw3q2V5uNY4tV2w.png","/icons/favicon.svg":"https://cdn-static-1.medium.com/_/fp/icons/favicon.KjTfUJo7yJH_fCoUzzH3cg.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":false,"domainCollectionSlug":"a-year-of-artificial-intelligence","algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"aadabda268e3bfaffe7b287709262fec","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","cssFontUrls":["https://cdn-static-1.medium.com/_/fp/css/fonts-base.by5Oi_VbnwEIvhnWIsuUjA.css"],"useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium.TAS6uQ-Y7kcKgi0xjcYHXw.ico"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"13f676ff00b7cff2\",\"ot-tracer-traceid\":\"5f5698d86aedfe52\",\"ot-tracer-sampled\":\"true\"}","host":"lightstep.medium.com"},"facebook":{"key":"542599432471018","secret":"c14df7146e9052a1131f3c900c1f0644","token":"542599432471018|1JqjIwxSfY9jOt_KwjWEl1R7T6I","namespace":"medium-com","scope":{"default":["public_profile","email","user_friends"],"connect":["public_profile","email","user_friends"],"login":["public_profile","email","user_friends"],"share":["public_profile","email","user_friends","publish_actions"]},"smartPublishWhitelistedPublications":["bcc38c8f6edf","f3726e2a5878","828a270689e","81c7d351c056","f30e42fd7ff8","8bf1d7d3081b","d16afa0ae7c","d8f3f6ad9c31","e74de0cedea9","15f753907972","c8c6a6b01ebd","3412b9729488","2ce4bbcf83bb","544c7006046e","7bfcdbc6b30a","a268fd916824","458a773bccd2"],"instantArticles":{"published":true,"developmentMode":false}},"mailingListArchiveUploadSizeMb":2,"isDoNotAuth":false}
// ]]></script><script charset="UTF-8" src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/main-base.js" async="true"></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"abf4609d4f9d","versionId":"fc8093d70faa","creatorId":"cb55958ea3bb","creator":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1474026363557,"imageId":"1*f_drXOgflIo8yaxCbSDOmQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"MCKapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":189,"usersFollowedByCount":374,"type":"SocialStats"},"social":{"userId":"lo_91b04744946","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"},"homeCollection":{"id":"bb87da25612c","name":"A year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING"],"creatorId":"cb55958ea3bb","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","shortDescription":"2016: Our venture into the mathematics, science, and…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":197,"postCount":6,"activeAt":1465125336105},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A year of Artificial Intelligence","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A year of Artificial Intelligence."},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":3,"postIds":[]}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"tintColor":"#FF16A085","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF049B80","point":0},{"color":"#FF188F77","point":0.1},{"color":"#FF20846E","point":0.2},{"color":"#FF247865","point":0.3},{"color":"#FF266C5B","point":0.4},{"color":"#FF266052","point":0.5},{"color":"#FF245447","point":0.6},{"color":"#FF21473D","point":0.7},{"color":"#FF1C3A32","point":0.8},{"color":"#FF162C26","point":0.9},{"color":"#FF0E1D19","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF16A085","point":0},{"color":"#FF40AB92","point":0.1},{"color":"#FF5BB69F","point":0.2},{"color":"#FF72C1AB","point":0.3},{"color":"#FF87CBB7","point":0.4},{"color":"#FF9AD5C3","point":0.5},{"color":"#FFADDFCF","point":0.6},{"color":"#FFBEE9DB","point":0.7},{"color":"#FFD0F2E6","point":0.8},{"color":"#FFE1FBF1","point":0.9},{"color":"#FFF1FFFD","point":1}],"backgroundColor":"#FF16A085"},"highlightSpectrum":{"colorPoints":[{"color":"#FFE1F9F0","point":0},{"color":"#FFDCF9EE","point":0.1},{"color":"#FFD6F8EC","point":0.2},{"color":"#FFD0F7EA","point":0.3},{"color":"#FFC9F7E8","point":0.4},{"color":"#FFC3F6E6","point":0.5},{"color":"#FFBCF6E3","point":0.6},{"color":"#FFB5F5E1","point":0.7},{"color":"#FFAEF5DF","point":0.8},{"color":"#FFA7F4DD","point":0.9},{"color":"#FF9FF4DB","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"fullTextRssFeed":0,"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"type":"Collection"},"homeCollectionId":"bb87da25612c","title":"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","detectedLanguage":"en","latestVersion":"fc8093d70faa","latestPublishedVersion":"fc8093d70faa","hasUnpublishedEdits":false,"latestRev":3532,"createdAt":1456559125318,"updatedAt":1463888425551,"acceptedAt":0,"firstPublishedAt":1457066600635,"latestPublishedAt":1463888425551,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"Do you know the chain rule? Then you know the neural network backpropagation algorithm!","bodyModel":{"paragraphs":[{"name":"4d27","type":4,"text":"","markups":[],"layout":5,"metadata":{"id":"1*pKv3DL-enonlNJxzqhlWkQ.jpeg","originalWidth":2750,"originalHeight":1833}},{"name":"ae68","type":3,"text":"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","markups":[]},{"name":"892d","type":13,"text":"Do you know the chain rule? Then you know the neural network backpropagation algorithm!","markups":[]},{"name":"e8ab","type":7,"text":"This is the first group (Lenny and Rohan) entry in our journey to extend our knowledge of Artificial Intelligence in the year of 2016. Learn more about our motives in this introduction post.","markups":[{"type":3,"start":25,"end":30,"anchorType":2,"userId":"de8e2540b759"},{"type":3,"start":35,"end":40,"anchorType":2,"userId":"cb55958ea3bb"},{"type":3,"start":55,"end":62,"href":"https://medium.com/a-year-of-artificial-intelligence","title":"","rel":"","anchorType":0},{"type":3,"start":172,"end":184,"href":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","title":"","rel":"","anchorType":0}]},{"name":"1716","type":1,"text":"In Rohan’s last post, he talked about evaluating and plugging holes in his knowledge of machine learning thus far. The backpropagation algorithm — the process of training a neural network — was a glaring one for both of us in particular. Together, we embarked on mastering backprop through some great online lectures from professors at MIT & Stanford. After attempting a few programming implementations and hand solutions, we felt equipped to write an article for AYOAI — together.","markups":[]},{"name":"d32d","type":1,"text":"Today, we’ll do our best to explain backpropagation and neural networks from the beginning. If you have an elementary understanding of differential calculus and perhaps an intuition of what machine learning is, we hope you come out of this blog post with an (acute, but existent nonetheless) understanding of neural networks and how to train them. Let us know if we succeeded!","markups":[]},{"name":"483f","type":13,"text":"Introduction to Neural Networks","markups":[]},{"name":"bbc0","type":1,"text":"Let’s start off with a quick introduction to the concept of neural networks. Fundamentally, neural networks are nothing more than really good function approximators — you give a trained network an input vector, it performs a series of operations, and it produces an output vector. To train our network to estimate an unknown function, we give it a collection of data points — which we denote the “training set” — that the network will learn from and generalize on to make future inferences.","markups":[]},{"name":"4df3","type":4,"text":"This is what a neural network looks like. Each circle is a neuron, and the arrows are connections between neurons in consecutive layers.","markups":[{"type":1,"start":59,"end":65},{"type":1,"start":129,"end":135}],"layout":1,"metadata":{"id":"0*m0tn6W0ipC3Ro7mo.","originalWidth":768,"originalHeight":362}},{"name":"44f8","type":1,"text":"Neural networks are structured as a series of layers, each composed of one or more neurons (as depicted above). Each neuron produces an output, or activation, based on the outputs of the previous layer and a set of weights.","markups":[{"type":1,"start":46,"end":52},{"type":1,"start":83,"end":90},{"type":1,"start":147,"end":157}]},{"name":"3850","type":4,"text":"This is how each neuron computes it’s own activation. It computes a weighted sum of the outputs of the previous layer (which, for the inputs, we’ll call x), and applies an activation function (more on that later), before forwarding it on to the next layer. Each neuron in a layer has its own set of weights — so while each neuron in a layer is looking at the same inputs, their outputs will all be different.","markups":[{"type":1,"start":153,"end":154}],"layout":1,"metadata":{"id":"1*OSspmzZwl9mK_wJgQDcyLA.png","originalWidth":406,"originalHeight":44}},{"name":"2003","type":1,"text":"When using a neural network to approximate a function, the data is forwarded through the network layer-by-layer until it reaches the final layer. The final layer’s activations are the predictions that the network actually makes.","markups":[]},{"name":"ff21","type":1,"text":"All this probably seems kind of magical, but it actually works. The key is finding the right set of weights for all of the connections to make the right decisions (this happens in a process known as training) — and that’s what most of this post is going to be about.","markups":[{"type":1,"start":100,"end":107},{"type":1,"start":199,"end":207}]},{"name":"0c95","type":1,"text":"When we’re training the network, it’s often convenient to have some metric of how good or bad we’re doing; we call this metric the cost function. Generally speaking, the cost function looks at the function the network has inferred and uses it to estimate values for the data points in our training set. The discrepancies between the outputs in the estimations and the training set data points are the principle values for our cost function. When training our network, the goal will be to get the value of this cost function as low as possible (we’ll see how to do that in just a bit, but for now, just focus on the intuition of what a cost function is and what it’s good for). Generally speaking, the cost function should be more or less convex, like so:","markups":[{"type":1,"start":721,"end":722},{"type":2,"start":715,"end":722}]},{"name":"6921","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*7i-bEOVyjyVmBOYV.","originalWidth":1130,"originalHeight":860}},{"name":"c1ed","type":1,"text":"In reality, it’s impossible for any network or cost function to be truly convex. However, as we’ll soon see, local minima may not be a big deal, as long as there is still a general trend for us to follow to get to the bottom. Also, notice that the cost function is parameterized by our network’s weights — we control our loss function by changing the weights.","markups":[]},{"name":"a604","type":1,"text":"One last thing to keep in mind about the loss function is that it doesn’t just have to capture how correctly your network estimates — it can specify any objective that needs to be optimized. For example, you generally want to penalize larger weights, as they could lead to overfitting. If this is the case, simply adding a regularization term to your cost function that expresses how big your weights will mean that, in the process of training your network, it will look for a solution that has the best estimates possible while preventing overfitting.","markups":[{"type":1,"start":323,"end":342}]},{"name":"d312","type":1,"text":"Now, let’s take a look at how we can actually minimize the cost function during the training process to find a set of weights that work the best for our objective.","markups":[]},{"name":"8e7d","type":13,"text":"Minimizing the Cost Function","markups":[]},{"name":"8a3a","type":1,"text":"Now that we’ve developed a metric for “scoring” our network (which we’ll denote as J(W)), we need to find the weights that will make that score as low as possible. If you think back to your pre-calculus days, your first instinct might be to set the derivative of the cost function to zero and solve, which would give us the locations of every minimum/maximum in the function. Unfortunately, there are a few problems with this approach:","markups":[{"type":1,"start":83,"end":87}]},{"name":"4842","type":10,"text":"We don’t have a simple equation for our cost function, so computing an expression for the derivative and solving it isn’t trivial.","markups":[]},{"name":"d97d","type":10,"text":"The function is many-dimensional (each weight gets its own dimension) — we need to find the points where all of those derivatives are zero. Also not so trivial.","markups":[]},{"name":"f51e","type":10,"text":"There are lots of minimums and maximums throughout the function, and sorting out which one is the one you should be using can be computationally expensive.","markups":[]},{"name":"17a6","type":1,"text":"Especially as the size of networks begins to scale up, solving for the weights directly becomes increasingly infeasible. Instead, we look at a different class of algorithms, called iterative optimization algorithms, that progressively work their way towards the optimal solution.","markups":[{"type":1,"start":181,"end":214}]},{"name":"d021","type":1,"text":"The most basic of these algorithms is gradient descent. Recall that our cost function will be essentially convex, and we want to get as close as possible to the global minimum. Instead of solving for it analytically, gradient descent follows the derivatives to essentially “roll” down the slope until it finds its way to the center.","markups":[{"type":1,"start":38,"end":54},{"type":2,"start":94,"end":105}]},{"name":"51b3","type":1,"text":"Let’s take the example of a single-weight neural network, whose cost function is depicted below.","markups":[]},{"name":"97f2","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*M0XaQitZPdQTCl86.","originalWidth":1130,"originalHeight":860}},{"name":"62e2","type":1,"text":"We start off by initializing our weight randomly, which puts us at the red dot on the diagram above. Taking the derivative, we see the slope at this point is a pretty big positive number. We want to move closer to the center — so naturally, we should take a pretty big step in the opposite direction of the slope.","markups":[]},{"name":"3159","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*lF6cKLbGItdwdKl-.","originalWidth":1130,"originalHeight":860}},{"name":"a94a","type":1,"text":"If we repeat the process enough, we soon find ourselves nearly at the bottom of our curve and much closer to the optimal weight configuration for our network.","markups":[]},{"name":"4332","type":1,"text":"More formally, gradient descent looks something like this:","markups":[]},{"name":"297c","type":4,"text":"This is the gradient descent update rule. It tells us how to update the weights of our network to get us closer to the minimum we’re looking for.","markups":[{"type":1,"start":29,"end":40}],"layout":1,"metadata":{"id":"1*U0VohJIM1F_bE5Msmjzv4Q.png","originalWidth":255,"originalHeight":91}},{"name":"75e5","type":1,"text":"Let’s dissect. Every time we want to update our weights, we subtract the derivative of the cost function w.r.t. the weight itself, scaled by a learning rate , and — that’s it! You’ll see that as it gets closer and closer to the center, the derivative term gets smaller and smaller, converging to zero as it approaches the solution. The same process applies with networks that have tens, hundreds, thousands, or more parameters — compute the gradient of the cost function w.r.t. each of the weights, and update each of your weights accordingly.","markups":[{"type":1,"start":143,"end":156}]},{"name":"712f","type":1,"text":"I do want to say a few more words on the learning rate, because it’s one of the more important hyperparameters (“settings” for your neural network) that you have control over. If the learning rate is too high, it could jump too far in the other direction, and you never get to the minimum you’re searching for. Set it too low, and your network will take ages to find the right weights, or it will get stuck in a local minimum. There’s no “magic number” to use when it comes to a learning rate, and it’s usually best to try several and pick the one that works the best for your individual network and dataset. In practice, many choose to anneal the learning rate over time — it starts out high, because it’s furthest from the solution, and decays as it gets closer.","markups":[{"type":1,"start":95,"end":110}]},{"name":"27d8","type":1,"text":"But as it turns out, gradient descent is kind of slow. Really slow, actually. Earlier I used the analogy of the weights “rolling” down the gradient to get to the bottom, but that doesn’t actually make any sense — it should pick up speed as it gets to the bottom, not slow down! Another iterative optimization algorithm, known as momentum, does just that. As the weights begin to “roll” down the slope, they pick up speed. When they get closer to the solution, the momentum that they picked up carries them closer to the optima while gradient descent would simply stop. As a result, training with momentum updates is both faster and can provide better results.","markups":[{"type":1,"start":329,"end":337}]},{"name":"e156","type":1,"text":"Here’s what the update rule looks like for momentum:","markups":[]},{"name":"da54","type":4,"text":"You might see the momentum update rule written differently depending on where you look, but the basic principle remains the same throughout.","markups":[],"layout":1,"metadata":{"id":"1*fO-eGqGcie5JQhaV5msoDw.png","originalWidth":259,"originalHeight":142}},{"name":"3b50","type":1,"text":"As we train, we accumulate a “velocity” value V. At each training step, we update V with the gradient at the current position (once again scaled by the learning rate). Also notice that, with each time step, we decay velocity V by a factor mu (usually somewhere around .9), so that over time we lose momentum instead of bouncing around by the minimum forever. We then update our weight in the direction of the velocity, and repeat the process again. Over the first few training iterations, V will grow as our weights “pick up speed” and take successively bigger leaps. As we approach the minimum, our velocity stops accumulating as quickly, and eventually begins to decay, until we’ve essentially reached the minimum. An important thing to note is that we accumulate a velocity independently for each weight — just because one weight is changing particularly clearly doesn’t mean any of the other weights need to be.","markups":[{"type":1,"start":46,"end":47},{"type":1,"start":82,"end":83},{"type":1,"start":239,"end":241},{"type":1,"start":489,"end":490}]},{"name":"de0e","type":1,"text":"There are lots of other iterative optimization algorithms that are commonly used with neural networks, but I won’t go into all of them here (if you’re curious, some of the more popular ones include Adagrad and Adam). The basic principle remains the same throughout — gradually update the weights to get them closer to the minimum. But regardless of which optimization algorithm you use, we still need to be able to compute the gradient of the cost function w.r.t. each weight. But our cost function isn’t a simple parabola anymore — it’s a complicated, many-dimensional function with countless local optima that we need to watch out for. That’s where backpropagation comes in.","markups":[{"type":3,"start":198,"end":205,"href":"http://www.magicbroom.info/Papers/DuchiHaSi10.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":210,"end":214,"href":"http://arxiv.org/abs/1412.6980","title":"","rel":"","anchorType":0}]},{"name":"e456","type":13,"text":"Before Backpropagation","markups":[]},{"name":"baac","type":1,"text":"The backpropagation algorithm was a major milestone in machine learning because, before it was discovered, optimization methods were extremely unsatisfactory. One popular method was to perturb (adjust) the weights in a random, uninformed direction (ie. increase or decrease) and see if the performance of the ANN increased. If it did not, one would attempt to either a) go in the other direction b) reduce the perturbation size or c) a combination of both. Another attempt was to use Genetic Algorithms (which became popular in AI at the same time) to evolve a high-performance neural network. In both cases, without (analytically) being informed on the correct direction, results and efficiency were suboptimal. This is where the backpropagation algorithm comes into play.","markups":[]},{"name":"e6d2","type":13,"text":"The Backpropagation Algorithm","markups":[]},{"name":"9613","type":1,"text":"Recall that, for any given supervised machine learning problem, we (aim to) select weights that provide the most optimal estimation of a function that models our training data. In other words, we want to find a set of weights W that minimizes on the output of J(W). We discussed the gradient descent algorithm — one where we update each weight by some negative, scalar reduction of the error derivative with respect to that weight. If we do choose to use gradient descent (or almost any other convex optimization algorithm), we need to find said derivatives in numerical form.","markups":[{"type":1,"start":226,"end":227},{"type":1,"start":260,"end":264}]},{"name":"1f67","type":1,"text":"For other machine learning algorithms like logistic regression or linear regression, computing the derivatives is an elementary application of differentiation. This is because the outputs of these models are just the inputs multiplied by some chosen weights, and at most fed through a single activation function (the sigmoid function in logistic regression). The same, however, cannot be said for neural networks. To demonstrate this, here is a diagram of a single-layered, shallow neural network:","markups":[]},{"name":"ee3f","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*Yk-ojFKLuPhZEHoU.","originalWidth":641,"originalHeight":138}},{"name":"16ea","type":1,"text":"As you can see, each neuron is a function of the previous one connected to it. In other words, if one were to change the value of w1, both “hidden 1” and “hidden 2” (and ultimately the output) neurons would change. Because of this notion of functional dependencies, we can mathematically formulate the output as an extensive composite function:","markups":[{"type":1,"start":130,"end":132},{"type":2,"start":150,"end":154}]},{"name":"e44e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*vbSngK4VQd0QFWgL.","originalWidth":341,"originalHeight":124}},{"name":"326e","type":1,"text":"And thus:","markups":[]},{"name":"643e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*MrJNRSWmwh8Yy-CG.","originalWidth":519,"originalHeight":33}},{"name":"bea4","type":1,"text":"Here, the output is a composite function of the weights, inputs, and activation function(s). It is important to realize that the hidden units/nodes are simply intermediary computations that, in actuality, can be reduced down to computations of the input layer.","markups":[]},{"name":"ca87","type":1,"text":"If we were to then take the derivative of said function with respect to some arbitrary weight (for example w1), we would iteratively apply the chain rule (which I’m sure you all remember from your calculus classes). The result would look similar to the following:","markups":[{"type":3,"start":143,"end":153,"href":"https://en.wikipedia.org/wiki/Chain_rule","title":"","rel":"","anchorType":0},{"type":1,"start":107,"end":109}]},{"name":"4ef6","type":4,"text":"If you fail to get an intuition of this, try researching about the chain rule.","markups":[],"layout":1,"metadata":{"id":"0*9jzf-PRocsKHtPCt.","originalWidth":784,"originalHeight":73}},{"name":"1191","type":1,"text":"Now, let’s attach a black box to the tail of our neural network. This black box will compute and return the error — using the cost function — from our output:","markups":[]},{"name":"2a52","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*SUsopH_FuWnAhQjD.","originalWidth":752,"originalHeight":144}},{"name":"9bde","type":1,"text":"All we’ve done is add another functional dependency; our error is now a function of the output and hence a function of the input, weights, and activation function. If we were to compute the derivative of the error with any arbitrary weight (again, we’ll choose w1), the result would be:","markups":[{"type":1,"start":261,"end":263}]},{"name":"b963","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*d5zULA4KVEsVYlng.","originalWidth":649,"originalHeight":79}},{"name":"4068","type":1,"text":"Each of these derivatives can be simplified once we choose an activation and error function, such that the entire result would represent a numerical value. At that point, any abstraction has been removed, and the error derivative can be used in gradient descent (as discussed earlier) to iteratively improve upon the weight. We compute the error derivatives w.r.t. every other weight in the network and apply gradient descent in the same way. This is backpropagation — simply the computation of derivatives that are fed to a convex optimization algorithm. We call it “backpropagation” because it almost seems as if we are traversing from the output error to the weights, taking iterative steps using chain the rule until we “reach” our weight.","markups":[{"type":2,"start":443,"end":448}]},{"name":"d24b","type":4,"text":"It’s like feed-forward… but the opposite! We take the derivative of J w.r.t. the output, the output w.r.t. the hidden units, then the final hidden unit w.r.t. the weight.","markups":[],"layout":1,"metadata":{"id":"0*k93UNqZ5FeScIVME.","originalWidth":752,"originalHeight":144}},{"name":"484a","type":1,"text":"When I first truly understood the backprop algorithm (just a couple of weeks ago), I was taken aback by how simple it was. Sure, the actual arithmetic/computations can be difficult, but this process is handled by our computers. In reality, backpropagation is just a rather tedious (but again, for a generalized implementation, computers will handle this) application of the chain rule. Since neural networks are convoluted multilayer machine learning model structures (at least relative to other ones), each weight “contributes” to the overall error in a more complex manner, and hence the actual derivatives require a lot of effort to produce. However, once we get past the calculus, backpropagation of neural nets is equivalent to typical gradient descent for logistic/linear regression.","markups":[]},{"name":"4d68","type":1,"text":"Thus far, I’ve walked through a very abstract form of backprop for a simple neural network. However, it is unlikely that you will ever use a single-layered ANN in applications. So, now, let’s make our black boxes — the activation and error functions — more concrete such that we can perform backprop on a multilayer neural net.","markups":[]},{"name":"06c4","type":1,"text":"Recall that our error function J(W) will compute the “error” of our neural network based on the output predictions it produces vs. the correct a priori outputs we know in our training set. More formally, if we denote our predicted output estimations as vector p, and our actual output as vector a, then we can use:","markups":[{"type":1,"start":31,"end":35},{"type":1,"start":260,"end":261},{"type":1,"start":295,"end":296},{"type":2,"start":143,"end":151}]},{"name":"7895","type":4,"text":"In this case, J need not be a function of W because p already is. We can use vector notation here because the inputs/outputs to our neural nets our vectors/some form of tensor.","markups":[{"type":1,"start":14,"end":16},{"type":1,"start":42,"end":44},{"type":1,"start":52,"end":54}],"layout":1,"metadata":{"id":"1*E1puoFN7OGdpwtxC2BxNfw.png","originalWidth":273,"originalHeight":73}},{"name":"85f3","type":1,"text":"This is just one example of a possible cost function (the log-likelihood is also a popular one), and we use it because of its mathematical convenience (this is a notion one will frequently encounter in machine learning): the squared expression exaggerates poor solutions and ensures each discrepancy is positive. It will soon become clear why we multiply the expression by half.","markups":[{"type":3,"start":58,"end":72,"href":"https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood","title":"","rel":"","anchorType":0}]},{"name":"ef2d","type":1,"text":"The derivative of the error w.r.t. the output was the first term in the error w.r.t. weight derivative expression we formulated earlier. Let’s now compute it!","markups":[]},{"name":"bdeb","type":4,"text":"With a simple application of the power and chain rule, our derivative is complete. The half gets cancelled due to the power rule.","markups":[],"layout":1,"metadata":{"id":"1*4WviflgVzi6jLFd31xfH5w.png","originalWidth":574,"originalHeight":86}},{"name":"f319","type":1,"text":"Our result is simply our predictions take away our actual outputs.","markups":[]},{"name":"9591","type":1,"text":"Now, let’s move on to the activation function. The activation function used depends on the context of the neural network. If we aren’t in a classification context, ReLU (Rectified Linear Unit, which is zero if input is negative, and the identity function when the input is positive) is commonly used today.","markups":[]},{"name":"6d90","type":4,"text":"The “Rectified Linear Unit” acivation function — http://i.stack.imgur.com/8CGlM.png","markups":[{"type":3,"start":49,"end":83,"href":"http://i.stack.imgur.com/8CGlM.png","title":"","rel":"nofollow","anchorType":0}],"layout":1,"metadata":{"id":"1*I3m4X5zMlOhhK8W3j3Pvkw.png","originalWidth":812,"originalHeight":612}},{"name":"2aab","type":1,"text":"If we’re in a classification context (that is, predicting on a discrete state with a probability ie. if an email is spam), we can use the sigmoid or tanh (hyperbolic tangent) function such that we can “squeeze” any value into the range 0 to 1. These are used instead of a typical step function because their “smoothness” properties allows for the derivatives to be non-zero. The derivative of the step function before and after the origin is zero. This will pose issues when we try to update our weights (nothing much will happen!).","markups":[{"type":3,"start":280,"end":293,"href":"https://en.wikipedia.org/wiki/Step_function","title":"","rel":"","anchorType":0},{"type":1,"start":418,"end":422}]},{"name":"818c","type":1,"text":"Now, let’s say we’re in a classification context and we choose to use the sigmoid function, which is of the following equation:","markups":[]},{"name":"4672","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*vT63OsGDWkOs3W8z.","originalWidth":317,"originalHeight":76}},{"name":"90ec","type":4,"text":"Smooth, continuous sigmoid function on the left. Step, piece-wise function on the right. https://en.wikibooks.org/wiki/File:HardLimitFunction.png & https://en.wikibooks.org/wiki/File:SigmoidFunction.png.","markups":[{"type":3,"start":89,"end":145,"href":"https://en.wikibooks.org/wiki/File:HardLimitFunction.png","title":"","rel":"","anchorType":0},{"type":3,"start":148,"end":202,"href":"https://en.wikibooks.org/wiki/File:SigmoidFunction.png","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"0*oaeR_qLU8v_oNr3G.","originalWidth":593,"originalHeight":225}},{"name":"7f19","type":1,"text":"As per usual, we’ll compute the derivative using differentiation rules as:","markups":[]},{"name":"4560","type":4,"text":"Looks confusing? Forgot differentiation? Don’t worry! Just take my word for it. It’s not necessary to have a complete mathematical comprehension of this derivation.","markups":[],"layout":1,"metadata":{"id":"0*2dW7Dvyhiop3GaVN.","originalWidth":800,"originalHeight":216}},{"name":"ba16","type":1,"text":"Sidenote: ReLU activation functions are also commonly used in classification contexts. There are downsides to using the sigmoid function — particularly the “vanishing gradient” problem — which you can read more about here.","markups":[{"type":3,"start":217,"end":221,"href":"https://www.quora.com/What-is-special-about-rectifier-neural-units-used-in-NN-learning","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":222}]},{"name":"f152","type":1,"text":"The sigmoid function is mathematically convenient (there it is again!) because we can represent its derivative in terms of the output of the function. Isn’t that cool‽","markups":[]},{"name":"dd9b","type":1,"text":"We are now in a good place to perform backpropagation on a multilayer neural network. Let me introduce you to the net we are going to work with:","markups":[]},{"name":"2060","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*m0tn6W0ipC3Ro7mo.","originalWidth":768,"originalHeight":362}},{"name":"89af","type":1,"text":"This net is still not as complex as one you may use in your programming, but its architecture allows us to nevertheless get a good grasp on backprop. In this net, we have 3 input neurons and one output neuron. There are four layers in total: one input, one output, and two hidden layers. There are 3 neurons in each hidden layer, too (which, by the way, need not be the case). The network is fully connected; there are no missing connections. Each neuron/node (save the inputs, which are usually pre-processed anyways) is an activity; it is the weighted sum of the previous neurons’ activities applied to the sigmoid activation function.","markups":[{"type":1,"start":525,"end":533}]},{"name":"29a6","type":1,"text":"To perform backprop by hand, we need to introduce the different variables/states at each point (layer-wise) in the neural network:","markups":[{"type":2,"start":84,"end":89}]},{"name":"b3a9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*JrJK5fbLAd5zum6h.","originalWidth":907,"originalHeight":365}},{"name":"41d2","type":1,"text":"It is important to note that every variable you see here is a generalization on the entire layer at that point. For example, when I say x_i, I am referring to the input to any input neuron (arbitrary value of i). I chose to place it in the middle of the layer for visibility purposes, but that does not mean that x_i refers to the middle neuron. I’ll demonstrate and discuss the implications of this later on.","markups":[{"type":1,"start":62,"end":77},{"type":1,"start":136,"end":139},{"type":1,"start":209,"end":210},{"type":1,"start":299,"end":302},{"type":1,"start":313,"end":316},{"type":2,"start":299,"end":302}]},{"name":"17bc","type":1,"text":"x refers to the input layer, y refers to hidden layer 1, z refers to hidden layer 2, and p refers to the prediction/output layer (which fits in nicely with the notation used in our cost function). If a variable has the subscript i, it means that the variable is the input to the relevant neuron at that layer. If a variable has the subscript j, it means that the variable is the output of the relevant neuron at that layer. For example, x_i refers to any input value we enter into the network. x_j is actually equal to x_i, but this is only because we choose not to use an activation function — or rather, we use the identity activation function — in the input layer’s activities. We only include these two separate variables to retain consistency. y_i is the input to any neuron in the first hidden layer; it is the weighted sum of all previous neurons (each neuron in the input layer multiplied by the corresponding connecting weights). y_j is the output of any neuron at the hidden layer, so it is equal to activation_function(y_i) = sigmoid(y_i) = sigmoid(weighted_sum_of_x_j). We can apply the same logic for z and p. Ultimately, p_j is the sigmoid output of p_i and hence is the output of the entire neural network that we pass to the error/cost function.","markups":[{"type":1,"start":0,"end":2},{"type":1,"start":29,"end":31},{"type":1,"start":57,"end":59},{"type":1,"start":89,"end":91},{"type":1,"start":229,"end":230},{"type":1,"start":342,"end":343},{"type":1,"start":437,"end":441},{"type":1,"start":494,"end":497},{"type":1,"start":519,"end":522},{"type":1,"start":749,"end":753},{"type":1,"start":939,"end":943},{"type":1,"start":1010,"end":1080},{"type":1,"start":1114,"end":1116},{"type":1,"start":1120,"end":1121},{"type":1,"start":1135,"end":1139},{"type":1,"start":1164,"end":1168},{"type":2,"start":266,"end":272},{"type":2,"start":379,"end":385}]},{"name":"cf41","type":1,"text":"The weights are organized into three separate variables: W1, W2, and W3. Each W is a matrix (if you are not comfortable with Linear Algebra, think of a 2D array) of all the weights at the given layer. For example, W1 are the weights that connect the input layer to the hidden layer 1. Wlayer_ij refers to any arbitrary, single weight at a given layer. To get an intuition of ij (which is really i, j), Wlayer_i are all the weights that connect arbitrary neuron i at a given layer to the next layer. Wlayer_ij (adding the j component) is the weight that connects arbitrary neuron i at a given layer to an arbitrary neuron j at the next layer. Essentially, Wlayer is a vector of Wlayer_is, which is a vector of real-valued Wlayer_ijs.","markups":[{"type":1,"start":57,"end":59},{"type":1,"start":61,"end":63},{"type":1,"start":69,"end":71},{"type":1,"start":78,"end":80},{"type":1,"start":214,"end":217},{"type":1,"start":285,"end":295},{"type":1,"start":375,"end":378},{"type":1,"start":395,"end":399},{"type":1,"start":402,"end":411},{"type":1,"start":461,"end":463},{"type":1,"start":499,"end":509},{"type":1,"start":521,"end":523},{"type":1,"start":579,"end":581},{"type":1,"start":621,"end":623},{"type":1,"start":655,"end":662},{"type":1,"start":677,"end":685},{"type":1,"start":721,"end":730}]},{"name":"964e","type":1,"text":"NOTE: Please note that the i’s and j’s in the weights and other variables are completely different. These indices do not correspond in any way. In fact, for x/y/z/p, i and j do not represent tensor indices at all, they simply represent the input and output of a neuron. Wlayer_ij represents an arbitrary weight at an index in a weight matrix, and x_j/y_j/z_j/p_j represent an arbitrary input/output point of a neuron unit.","markups":[{"type":1,"start":0,"end":6},{"type":1,"start":27,"end":28},{"type":1,"start":30,"end":31},{"type":1,"start":35,"end":36},{"type":1,"start":78,"end":98},{"type":1,"start":157,"end":164},{"type":1,"start":166,"end":168},{"type":1,"start":172,"end":174},{"type":1,"start":270,"end":280},{"type":1,"start":347,"end":363},{"type":2,"start":78,"end":98}]},{"name":"59cc","type":1,"text":"That last part about weights was tedious! It’s crucial to understand how we’re separating the neural network here, especially the notion of generalizing on an entire layer, before moving forward.","markups":[]},{"name":"6a70","type":1,"text":"To acquire a comprehensive intuition of backpropagation, we’re going to backprop this neural net as discussed before. More specifically, we’re going to find the derivative of the error w.r.t. an arbitrary weight in the input layer (W1_ij). We could find the derivative of the error w.r.t. an arbitrary weight in the first or second hidden layer, but let’s go as far back as we can; the more backprop, the better!","markups":[{"type":1,"start":161,"end":231},{"type":1,"start":232,"end":237}]},{"name":"c6dd","type":1,"text":"So, mathematically, we are trying to obtain (to perform our iterative optimization algorithm with):","markups":[]},{"name":"4412","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*QMwl-OVNNQ8KZVTv.","originalWidth":196,"originalHeight":87}},{"name":"9dd3","type":1,"text":"We can express this graphically/visually, using the same principles as earlier (chain rule), like so:","markups":[]},{"name":"83d0","type":4,"text":"We backpropagate from the error all the way to the weights in the input layer. Note that the weight itself is any arbitrary one in that layer. In a fully connected neural net, we can make these generalizations considering we are consistent with our indices.","markups":[{"type":1,"start":114,"end":123},{"type":1,"start":148,"end":164}],"layout":1,"metadata":{"id":"0*HLo6XxRwCzjUzXXh.","originalWidth":907,"originalHeight":365}},{"name":"e852","type":1,"text":"In two layers, we have three red lines pointing in three different directions, instead of just one. This is a reinforcement of (and why it is important to understand) the fact that variable j is just a generalization/represents any point in the layer. So, when we differentiate p_i with respect to the layer before that, there are three different weights, as I hope you can see, in W3_ij that contribute to the value p_i. There also happen to be three weights in W3 in total, but this isn’t the case for the layers before; it is only the case because layer p has one neuron — the output — in it. We stop backprop at the input layer and so we just point to the single weight we are looking for.","markups":[{"type":1,"start":181,"end":192},{"type":1,"start":278,"end":281},{"type":1,"start":331,"end":354},{"type":1,"start":382,"end":387},{"type":1,"start":417,"end":420},{"type":1,"start":463,"end":465},{"type":1,"start":557,"end":559}]},{"name":"3587","type":1,"text":"Wonderful! Now let’s work out all this great stuff mathematically. Immediately, we know:","markups":[]},{"name":"bee9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*DQcoyR-fFf2ieOBo.","originalWidth":357,"originalHeight":93}},{"name":"29fa","type":1,"text":"We have already established the left hand side, so now we just need to use the chain rule to simplify it further. The derivative of the error w.r.t. the weight can be written as the derivative of the error w.r.t. the output prediction multiplied by the derivative of the output prediction w.r.t. the weight. At this point, we’ve traversed one red line back. We know this because","markups":[]},{"name":"a783","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*vRuT30CH_1RLY7MQ.","originalWidth":56,"originalHeight":86}},{"name":"d8d2","type":1,"text":"is reducible to a numerical value. Specifically, the derivative of the error w.r.t. the output prediction is:","markups":[]},{"name":"1d98","type":4,"text":"We know this from our manual derivation earlier.","markups":[],"layout":1,"metadata":{"id":"1*uVimZGqI0FPVXXb3oe9BWw.png","originalWidth":99,"originalHeight":53}},{"name":"5256","type":1,"text":"Hence:","markups":[]},{"name":"05af","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*nZwutmypOiiEooG8kxorKg.png","originalWidth":279,"originalHeight":93}},{"name":"88c4","type":1,"text":"Going one more layer backwards, we can determine that:","markups":[]},{"name":"efaf","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*94LsJGVc_g4q66-U.","originalWidth":235,"originalHeight":93}},{"name":"ecae","type":1,"text":"In other words, the derivative of the output prediction w.r.t. the weight is the derivative of the output w.r.t. the input to the output layer (p_i) multiplied by the derivative of that value w.r.t. the weight. This represents our second red line. We can solve the first term like so:","markups":[{"type":1,"start":144,"end":147}]},{"name":"9739","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*ggH61s46zIpxtLV_.","originalWidth":211,"originalHeight":88}},{"name":"da49","type":1,"text":"This corresponds with the derivative of the sigmoid function we solved earlier, which was equal to the output multiplied by one minus the output. In this case, p_j is the output of the sigmoid function. Now, we have:","markups":[{"type":1,"start":160,"end":164},{"type":2,"start":164,"end":167}]},{"name":"4a10","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*prpHixS0PhAhxjG3.","originalWidth":303,"originalHeight":131}},{"name":"b277","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*fGy8zY6O7zWES2IS7wXu_g.png","originalWidth":441,"originalHeight":89}},{"name":"3e70","type":1,"text":"Let’s move on to the third red line(s). This one is interesting because we begin to “spread” out. Since there are multiple different weights that contribute to the value of p_i, we need to take into account their individual “pull” factors into our derivative:","markups":[{"type":1,"start":173,"end":176}]},{"name":"7455","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*N0RhnqpTNMF3t_1B.","originalWidth":272,"originalHeight":93}},{"name":"e609","type":1,"text":"If you’re a mathematician, this notation may irk you slightly; sorry if that’s the case! In computer science, we tend to stray from the notion of completely legal mathematical expressions. This is yet again again another reason why it’s key to understand the role of layer generalization; z_j here is not just referring to the middle neuron, it’s referring to an arbitrary neuron. The actual value of j in the summation is not changing (it’s not even an index or a value in the first place), and we don’t really consider it. It’s less of a mathematical expression and more of a statement that we will iterate through each generalized neuron z_j and use it. In other words, we iterate over the derivative terms and sum them up using z_1, z_2, and z_3. Before, we could write p_j as any single value because the output layer just contains one node; there is just one p_j. But we see here that this is no longer the case. We have multiple z_j values, and p_i is functionally dependent on each of these z_j values. So, when we traverse from p_j to the preceding layer, we need to consider each contribution from layer z to p_j separately and add them up to create one total contribution. There’s no upper bound to the summation; we just assume that we start at zero and end at our maximum value for the number of neurons in the layer. Please again note that the same changes are not reflected in W1_ij, where j refers to an entirely different thing. Instead, we’re just stating that we will use the different z_j neurons in layer z.","markups":[{"type":1,"start":289,"end":292},{"type":1,"start":401,"end":402},{"type":1,"start":578,"end":587},{"type":1,"start":641,"end":645},{"type":1,"start":732,"end":735},{"type":1,"start":737,"end":740},{"type":1,"start":746,"end":749},{"type":1,"start":774,"end":778},{"type":1,"start":865,"end":868},{"type":1,"start":936,"end":939},{"type":1,"start":951,"end":955},{"type":1,"start":999,"end":1002},{"type":1,"start":1036,"end":1040},{"type":1,"start":1114,"end":1115},{"type":1,"start":1119,"end":1122},{"type":1,"start":1392,"end":1397},{"type":1,"start":1405,"end":1406},{"type":1,"start":1445,"end":1446},{"type":1,"start":1466,"end":1474},{"type":1,"start":1505,"end":1509},{"type":1,"start":1526,"end":1527},{"type":2,"start":392,"end":398},{"type":2,"start":578,"end":587},{"type":2,"start":1466,"end":1474}]},{"name":"cdf7","type":1,"text":"Since p_j is a summation of each weight multiplied by each z_j (weighted sum), if we were to take the derivative of p_j with any arbitrary z_j, the result would be the connecting weight since said weight would be the coefficient of the term (derivative of m*x w.r.t. x is just m):","markups":[{"type":1,"start":6,"end":9},{"type":1,"start":59,"end":63},{"type":1,"start":116,"end":120},{"type":1,"start":139,"end":142}]},{"name":"cd24","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*nGZuc0J3Wc6oZWmn.","originalWidth":134,"originalHeight":88}},{"name":"0ef7","type":1,"text":"W3_ij is loosely defined here. ij still refers to any arbitrary weight — where ij are still separate from the j used in p_i/z_j — but again, as computer scientists and not mathematicians, we need not be pedantic about the legality and intricacy of expressions; we just need an intuition of what the expressions imply/mean. It’s almost a succinct form of psuedo-code! So, even though this defines an arbitrary weight, we know it means the connecting weight. We can also see this from the diagram: when we walk from p_j to an arbitrary z_j, we walk along the connecting weight. So now, we have:","markups":[{"type":1,"start":0,"end":5},{"type":1,"start":31,"end":34},{"type":1,"start":79,"end":82},{"type":1,"start":110,"end":112},{"type":1,"start":120,"end":127},{"type":1,"start":438,"end":449},{"type":1,"start":514,"end":518},{"type":1,"start":534,"end":537}]},{"name":"530e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*w-inVp_X6HASLTdv.","originalWidth":250,"originalHeight":93}},{"name":"dcb9","type":1,"text":"At this point, I like to continue playing the “reduction test”. The reduction test states that, if we can further simplify a derivative term, we still have more backprop to do. Since we can’t yet quite put the derivative of z_j w.r.t. W1_ij into a numerical term, let’s keep going (and fast-forward a bit). Using chain rule, we follow the fourth line back to determine that:","markups":[{"type":1,"start":224,"end":227},{"type":1,"start":235,"end":240}]},{"name":"4d77","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*SdmtQirS-Dj_O77y.","originalWidth":469,"originalHeight":93}},{"name":"7386","type":1,"text":"Since z_j is the sigmoid of z_i, we use the same logic as the previous layer and apply the sigmoid derivative. The derivative of z_i w.r.t. W1_ij, demonstrated by the fifth line(s) back, requires the same idea of “spreading out” and summation of contributions:","markups":[{"type":1,"start":6,"end":10},{"type":1,"start":28,"end":31},{"type":1,"start":129,"end":133},{"type":1,"start":140,"end":145}]},{"name":"627e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*KpJrA6EpVWN2Eo2z.","originalWidth":468,"originalHeight":93}},{"name":"e9fc","type":1,"text":"Briefly, since z_i is the weighted sum of each y_j in y, we sum over the derivatives which, similar to before, simplifies to the relevant connecting weights in the preceding layer (W2 in this case).","markups":[{"type":1,"start":15,"end":19},{"type":1,"start":47,"end":51},{"type":1,"start":54,"end":55},{"type":1,"start":181,"end":183}]},{"name":"71de","type":1,"text":"We’re almost there, let’s go further; there’s still more reduction to do:","markups":[]},{"name":"b3f1","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*ia5ppPK0yG1f-8WQ.","originalWidth":459,"originalHeight":93}},{"name":"3673","type":1,"text":"We have, of course, another sigmoid activation function to deal with. This is the sixth red line. Notice, now, that we have just one line remaining. In fact, our last derivative term here passes (or rather, fails) the reduction test! The last line traverses from the input at y_i to x_j, walking along W1_ij. Wait a second — is this not what we are attempting to backprop to? Yes, it is! Since we are, for the first time, directly deriving y_i w.r.t. the weight W1_ij, we can think of the coefficient of W1_ij as being x_j in our weighted sum (instead of the vice versa as used previously). Hence, the simplification follows:","markups":[{"type":1,"start":276,"end":280},{"type":1,"start":283,"end":286},{"type":1,"start":302,"end":307},{"type":1,"start":440,"end":443},{"type":1,"start":462,"end":467},{"type":1,"start":504,"end":510},{"type":1,"start":519,"end":523},{"type":2,"start":129,"end":133},{"type":2,"start":422,"end":431}]},{"name":"b748","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*GfdjMI9YPac0G9Ix.","originalWidth":175,"originalHeight":89}},{"name":"706b","type":1,"text":"Of course, since each x_j in layer x contributes to the weighted sum y_i, we sum over the effects. And that’s it! We can’t reduce any further from here. Now, let’s tie all these individual expressions together:","markups":[{"type":1,"start":22,"end":26},{"type":1,"start":35,"end":37},{"type":1,"start":69,"end":72}]},{"name":"5da6","type":4,"text":"Our final expression for the derivative of the error w.r.t. any weight in W1","markups":[{"type":1,"start":74,"end":76}],"layout":1,"metadata":{"id":"1*UY2yJCSfx5y-zzlNAIWrSQ.png","originalWidth":993,"originalHeight":94}},{"name":"5cb5","type":1,"text":"With no more partial derivative terms left, our work is complete! This gives us the derivative of the error w.r.t. any arbitrary weight in the input layer/W1. That was a lot of work — maybe now we can sympathize with the poor computers!","markups":[{"type":1,"start":155,"end":157}]},{"name":"ad17","type":1,"text":"Something you should notice is that values such as p_j, a, z_j, y_j, x_j etc. are the values of the network at the different points. But where do they come from? Actually, we would need to perform a feed-forward of the neural network first and then capture these variables.","markups":[{"type":1,"start":51,"end":73}]},{"name":"eefb","type":13,"text":"Neural Network Training Overview","markups":[]},{"name":"abbb","type":1,"text":"Our task is to now perform Gradient Descent to train the neural net:","markups":[]},{"name":"e232","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*B_2wAzU14ush5uBO.","originalWidth":417,"originalHeight":183}},{"name":"858b","type":1,"text":"We perform gradient descent on each weight in each layer. Notice that the resulting gradient should change each time because the weight itself changes, (and as a result, the performance and output of the entire net should change) even if it’s a small perturbation. This means that, at each update, we need to do a feed-forward of the neural net. Not just once before, but once each iteration.","markups":[{"type":2,"start":377,"end":391}]},{"name":"9d72","type":1,"text":"These are then the steps to train an entire neural network:","markups":[]},{"name":"1433","type":10,"text":"Create our connected neural network and prepare training data","markups":[]},{"name":"83ad","type":10,"text":"Initialize all the weights to random values","markups":[]},{"name":"6518","type":1,"text":"It’s important to note that one must not initialize the weights to zero, similar to what may be done in other machine learning algorithms. If weights are initialized to zero, after each update, the outgoing weights of each neuron will be identical, because the gradients will be identical (which can be proved). Because of this, the proceeding hidden units will remain the same value and will continue to follow each other. Ultimately, this means that our training will become extremely constrained (due to the “symmetry”), and we won’t be able to build interesting functions. Also, neural networks may get stuck at local optima (places where the gradient is zero but are not the global minima), so random weight initialization allows one to hopefully have a chance of circumventing that by starting at many different random values.","markups":[{"type":2,"start":150,"end":154}]},{"name":"c8d5","type":1,"text":"3. Perform one feed-forward using the training data","markups":[]},{"name":"248d","type":1,"text":"4. Perform backpropagation to get the error derivatives w.r.t. each and every weight in the neural network","markups":[]},{"name":"e96a","type":1,"text":"5. Perform gradient descent to update each weight by the negative scalar reduction (w.r.t. some learning rate alpha) of the respective error derivative. Increment the number of iterations.","markups":[]},{"name":"1d1d","type":1,"text":"6. If we have converged (in reality, though, we just stop when we have reached the number of maximum iterations) training is complete. Else, repeat starting at step 3.","markups":[]},{"name":"26b0","type":1,"text":"If we initialize our weights randomly (and not to zero) and then perform gradient descent with derivatives computed from backpropagation, we should expect to train a neural network in no time! I hope this example brought clarity to how backprop works and the intuition behind it. If you didn’t understand the intricacies of the example but understand and appreciate the concept of backprop as a whole, you’re still in a great place! Next we’ll go ahead and explain backprop code that works on any generalized architecture of a neural network using the ReLU activation function.","markups":[]},{"name":"ef5a","type":13,"text":"Implementing Backpropagation","markups":[]},{"name":"6a97","type":1,"text":"Now that we’ve developed the math and intuition behind backpropagation, let’s try to implement it. We’ll divide our implementation into three distinct steps:","markups":[]},{"name":"5493","type":10,"text":"Feed-forward. In this step, we take the inputs and forward them through the network, layer by layer, to generate the output activations (as well as all of the activations in the hidden layers). When we are actually using our network (rather than training it), this is the only step we’ll need to perform.","markups":[{"type":1,"start":0,"end":12}]},{"name":"2aa2","type":10,"text":"Backpropagation. Here, we’ll take our error function and compute the weight gradients for each layer. We’ll use the algorithm just described to compute the derivative of the cost function w.r.t. each of the weights in our network, which will in turn allow us to complete step 3.","markups":[{"type":1,"start":0,"end":15}]},{"name":"e69d","type":10,"text":"Weight update. Finally, we’ll use the gradients computed in step 2 to update our weights. We can use any of the update rules discussed previously during this step (gradient descent, momentum, and so on).","markups":[{"type":1,"start":0,"end":13}]},{"name":"88ea","type":1,"text":"Let’s start off by defining what the API we’re implementing looks like. We’ll define our network as a series of Layer instances that our data passes through — this means that instead of modeling each individual neuron, we group neurons from a single layer together. This makes it a bit easier to reason about larger networks, but also makes the actual computations faster (as we’ll see shortly). Also — we’re going to write the code in Python.","markups":[]},{"name":"6860","type":1,"text":"Each layer will have the following API:","markups":[]},{"name":"d16f","type":8,"text":"class ReLULayer(object):\n    def __init__(self, size_in, size_out):\n        # Initialize this layer with random weights and any other  \n        # parameters we may need.\n        pass","markups":[]},{"name":"4476","type":8,"text":"    def forward(self, in_act):\n        # Compute the outgoing activations from this layer, given \n        # the activations from the previous layer.\n        pass","markups":[]},{"name":"3b56","type":8,"text":"    def backward(self, out_grad):\n        # out_grad is the derivative of the cost function w.r.t. the \n        # inputs to all of the neurons for the following layer. We \n        # need to compute the gradient of our own weights, and \n        # return another the gradient of the inputs to this layer to \n        # continue the backpropagation.\n        pass","markups":[]},{"name":"dcbd","type":8,"text":"    def update(self, alpha):\n        # Perform the actual weight update step.\n        pass","markups":[]},{"name":"26ef","type":1,"text":"(This isn’t great API design — ideally, we would decouple the backprop and weight update from the rest of the object, so the specific algorithm we use for updating weights isn’t tied to the layer itself. But that’s not the point, so we’ll stick with this design for the purposes of explaining how backpropagation works in a real-life scenario. Also: we’ll be using numpy throughout the implementation. It’s an awesome tool for mathematical operations in Python (especially tensor based ones), but we don’t have the time to get into how it works — if you want a good introduction, here ya’ go.)","markups":[{"type":3,"start":365,"end":370,"href":"http://www.numpy.org/","title":"","rel":"","anchorType":0},{"type":3,"start":580,"end":591,"href":"http://cs231n.github.io/python-numpy-tutorial/","title":"","rel":"","anchorType":0}]},{"name":"bdcd","type":1,"text":"We can start by implementing the weight initialization. As it turns out, how you initialize your weights is actually kind of a big deal for both network performance and convergence rates. Here’s how we’ll initialize our weights:","markups":[]},{"name":"379d","type":8,"text":"self.W = np.random.randn(self.size_in, self.size_out) * np.sqrt(2.0/self.size_in)","markups":[]},{"name":"c4b1","type":1,"text":"This initializes a weight matrix of the appropriate dimensions with random values sampled from a normal distribution. We then scale it rad(2/self.size_in), giving us a variance of 2/self.size_in (derivation here).","markups":[{"type":3,"start":207,"end":211,"href":"http://arxiv-web3.library.cornell.edu/abs/1502.01852","title":"","rel":"","anchorType":0}]},{"name":"7407","type":1,"text":"And that’s all we need for layer initialization! Let’s move on to implementing our first objective — feed-forward. This is actually pretty simple — a dot product of our input activations with the weight matrix, followed by our activation function, will give us the activations we need. The dot product part should make intuitive sense; if it doesn’t, you should sit down and try to work through it on a piece of paper. This is where the performance gains of grouping neurons into layers comes from: instead of keeping an individual weight vector for each neuron, and performing a series of vector dot products, we can just do a single matrix operation (which, thanks to the wonders of modern processors, is significantly faster). In fact, we can compute all of the activations from a layer in just two lines:","markups":[]},{"name":"dd0f","type":8,"text":"# Compute weighted sum for each neuron\nself.out_act = np.dot(self.in_act, self.W)","markups":[]},{"name":"4eae","type":8,"text":"# Activation function (any sum \x3c 0 is capped at 0)\nself.out_act[self.out_act \x3c 0] = 0","markups":[]},{"name":"6796","type":8,"text":"return self.out_act","markups":[]},{"name":"1674","type":1,"text":"Simple enough. Let’s move on to backpropagation.","markups":[]},{"name":"3229","type":1,"text":"This one’s a bit more involved. First, we compute the derivative of the output w.r.t. the weights, then the derivative of the cost w.r.t. the output, followed by chain rule to get the derivative of the cost w.r.t. the weights.","markups":[]},{"name":"101b","type":1,"text":"Let’s start with the first part — the derivative of the output w.r.t. the weights. That should be simple enough; because you’re multiplying the weight by the corresponding input activation, the derivative will just be the corresponding input activation.","markups":[]},{"name":"93aa","type":8,"text":"output_wrt_weights = np.ones(self.W.shape) * self.in_act[:, None]","markups":[]},{"name":"5081","type":1,"text":"Except, because we’re using the ReLU activation function, the weights have no effect if the corresponding output is \x3c 0 (because it gets capped anyway). This should take care of that hiccup:","markups":[]},{"name":"dcc6","type":8,"text":"output_wrt_weights[:, self.out_act \x3c 0] = 0","markups":[]},{"name":"8476","type":1,"text":"(More formally, you’re multiplying by the derivative of the activation function, which is 0 when the activation is \x3c 0 and 1 elsewhere.)","markups":[]},{"name":"1c81","type":1,"text":"Let’s take a brief detour to talk about the out_grad parameter that our backward method gets. Let’s say we have a network with two layers: the first has m neurons, and the second has n. Each of the m neurons produces an activation, and each of the n neurons looks at each of the m activations. The out_grad parameter is an m x n matrix of how each m affects each of the n neurons it feeds into.","markups":[{"type":1,"start":44,"end":52},{"type":1,"start":72,"end":80},{"type":1,"start":153,"end":154},{"type":1,"start":183,"end":184},{"type":1,"start":198,"end":200},{"type":1,"start":248,"end":249},{"type":1,"start":279,"end":281},{"type":1,"start":298,"end":307},{"type":1,"start":323,"end":325},{"type":1,"start":327,"end":329},{"type":1,"start":348,"end":349},{"type":1,"start":370,"end":371}]},{"name":"1897","type":1,"text":"Now, we need the derivative of the cost w.r.t. each of the outputs — which is essentially the out_grad parameter we’re given! We just need to sum up each row of the matrix we’re given, as per the backpropagation formula.","markups":[{"type":1,"start":94,"end":102}]},{"name":"8c3d","type":8,"text":"cost_wrt_output = np.sum(np.atleast_2d(grad), axis=1)","markups":[]},{"name":"6f19","type":1,"text":"Finally, we end up with something like this:","markups":[]},{"name":"ae00","type":8,"text":"self.dW = cost_wrt_weights","markups":[]},{"name":"1940","type":1,"text":"Now, we need to compute the derivative of our inputs to pass along to the next layer. We can perform a similar chain rule — derivative of the output w.r.t. the inputs times the derivative of the cost w.r.t. the outputs.","markups":[]},{"name":"a3ee","type":8,"text":"output_wrt_inputs = self.W\noutput_wrt_inputs[:, self.out_act \x3c 0] = 0","markups":[]},{"name":"8547","type":8,"text":"cost_wrt_inputs = cost_wrt_output * output_wrt_inputs","markups":[]},{"name":"5d7b","type":8,"text":"return cost_wrt_inputs","markups":[]},{"name":"c4f3","type":1,"text":"And that’s it for the backpropagation step.","markups":[]},{"name":"6e9c","type":1,"text":"The final step is the weight update. Assuming we’re sticking with gradient descent for this example, this can be a simple one-liner:","markups":[]},{"name":"c540","type":8,"text":"self.W = self.W — self.dW * alpha","markups":[]},{"name":"07ec","type":1,"text":"To actually train our network, we take one of our training samples and call forward on each layer consecutively, passing the output of the previous layer as the input of the following layer. We compute dJ, passing that as the out_grad parameter to the last layer’s backward method. We call backward on each of the layers in reverse order, this time passing the output of the further layer as out_grad to the previous layer. Finally, we call update on each of our layers and repeat.","markups":[{"type":1,"start":76,"end":83},{"type":1,"start":226,"end":234},{"type":1,"start":265,"end":273},{"type":1,"start":290,"end":298},{"type":1,"start":392,"end":401},{"type":1,"start":441,"end":448}]},{"name":"7ee1","type":1,"text":"There’s one last detail that we should include, which is the concept of a bias (akin to that of a constant term in any given equation). Notice that, with our current implementation, the activation of a neuron is determined solely based on the activations of the previous layer. There’s no bias term that can shift the activation up or down independent of the inputs. A bias term isn’t strictly necessary — in fact, if you train your network as-is, it would probably still work fine. But if you do need a bias term, the code stays almost the same — the only difference is that you need to add a column of 1s to the incoming activations, and update your weight matrix accordingly, so one of your weights gets treated as a bias term. The only other difference is that, when returning cost_wrt_inputs, you can cut out the first row — nobody cares about the gradients associated with the bias term because the previous layer has no say in the activation of the bias neuron.","markups":[{"type":1,"start":74,"end":79},{"type":1,"start":781,"end":796}]},{"name":"b1b8","type":1,"text":"Implementing backpropagation can be kind of tricky, so it’s often a good idea to check your implementation. You can do so by computing the gradient numerically (by literally perturbing the weight and calculating the difference in your cost function) and comparing it to your backpropagation-computed gradient. This gradient check doesn’t need to be run once you’ve verified your implementation, but it could save a lot of time tracking down potential problems with your network.","markups":[{"type":1,"start":315,"end":329}]},{"name":"feed","type":1,"text":"Nowadays, you often don’t even need to implement a neural network on your own, as libraries such as Caffe, Torch, or TensorFlow will have implementations ready to go. That being said, it’s often a good idea to try implementing it on your own to get a better grasp of how everything works under the hood.","markups":[{"type":3,"start":100,"end":105,"href":"http://caffe.berkeleyvision.org/","title":"","rel":"","anchorType":0},{"type":3,"start":107,"end":112,"href":"http://torch.ch/","title":"","rel":"","anchorType":0},{"type":3,"start":117,"end":127,"href":"http://tensorflow.org","title":"","rel":"","anchorType":0}]},{"name":"c3a6","type":13,"text":"Learning More about Neural Networks","markups":[]},{"name":"38e2","type":1,"text":"Intrigued? Looking to learn more about neural networks? Here are some great online classes to get you started:","markups":[]},{"name":"bf46","type":1,"text":"Stanford’s CS231n. Although it’s technically about convolutional neural networks, the class provides an excellent introduction to and survey of neural networks in general. Class videos, notes, and assignments are all posted here, and if you have the patience for it I would strongly recommend walking through the assignments so you can really get to know what you’re learning.","markups":[{"type":3,"start":0,"end":17,"href":"http://cs231n.stanford.edu","title":"","rel":"","anchorType":0},{"type":3,"start":224,"end":228,"href":"http://cs231n.stanford.edu/syllabus.html","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":17}]},{"name":"b3f7","type":1,"text":"MIT 6.034. This class, taught by Prof. Patrick Henry Winston, explores many different algorithms and disciplines in Artificial Intelligence. There’s a great lecture on backprop that I actually used as a stepping stone to getting setup writing this article. I also learned genetic algorithms from Prof. Winston — he’s a great teacher!","markups":[{"type":3,"start":0,"end":9,"href":"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/","title":"","rel":"","anchorType":0},{"type":3,"start":157,"end":164,"href":"https://www.youtube.com/watch?v=q0pm3BrIUFo","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":9}]},{"name":"39ed","type":1,"text":"We hope that, if you visited this article without knowing how the backpropagation algorithm works, you are reading this with an (at least rudimentary) mathematical or conceptual intuition of it. Writing and conveying such a complex algorithm to a supposed beginner has proven to be an extremely difficult task for us, but it’s helped us truly understand what we’ve been learning about. With greater knowledge in a fundamental area of machine learning, we are now excited to take a look at new, interesting algorithms and disciplines in the field. We are looking forward to continue documenting these endeavors together.","markups":[{"type":1,"start":610,"end":618}],"hasDropCap":true}],"sections":[{"name":"5f42","startIndex":0},{"name":"bf29","startIndex":3},{"name":"6590","startIndex":4},{"name":"caa9","startIndex":186}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","createdAtRelative":"7 months ago","updatedAtRelative":"4 months ago","acceptedAtRelative":"","createdAtEnglish":"February 26, 2016","updatedAtEnglish":"May 21, 2016","acceptedAtEnglish":"","firstPublishedAtEnglish":"March 3, 2016","latestPublishedAtEnglish":"May 21, 2016","allowNotes":true,"snippet":"Do you know the chain rule? Then you know the neural network backpropagation algorithm!","previewImage":{"imageId":"1*pKv3DL-enonlNJxzqhlWkQ.jpeg","filter":"","backgroundSize":"","originalWidth":2750,"originalHeight":1833,"strategy":"resample","height":0,"width":0},"wordCount":7088,"imageCount":42,"readingTime":29.597169811320757,"subtitle":"Do you know the chain rule? Then you know the neural network backpropagation algorithm!","publishedInCount":1,"usersBySocialRecommends":[],"latestPublishedAtAbbreviated":"May 21","firstPublishedAtAbbreviated":"Mar 3","emailSnippet":"This is the first group (Lenny and Rohan) entry in our journey to extend our knowledge of Artificial Intelligence in the year of 2016. Learn more about our motives in this introduction post.\n\nIn Rohan’s last post, he talked about evaluating and plugging holes in his knowledge of machine learning thus far. The backpropagation algorithm — the process of training a neural network — was a glaring one for both of us in particular.","recommends":34,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"neural-networks","name":"Neural Networks","postCount":284,"virtuals":{"isFollowing":false},"metadata":{"followerCount":643,"postCount":284,"coverImage":{"id":"1*NFLAAEvw8CrIBOJRzimB-Q.png","originalWidth":590,"originalHeight":394}},"type":"Tag"},{"slug":"machine-learning","name":"Machine Learning","postCount":4678,"virtuals":{"isFollowing":false},"metadata":{"followerCount":7310,"postCount":4678,"coverImage":{"id":"1*Oi6D1dlvnGd-4Ano968Lsw.jpeg","originalWidth":2912,"originalHeight":1900}},"type":"Tag"},{"slug":"algorithms","name":"Algorithms","postCount":896,"virtuals":{"isFollowing":false},"metadata":{"followerCount":818,"postCount":896,"coverImage":{"id":"1*1-5hf_o6j3szhVe33L104A.jpeg","originalWidth":2700,"originalHeight":1800}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":1,"links":{"entries":[{"url":"https://medium.com/a-year-of-artificial-intelligence","alts":[{"type":2,"url":"medium://a-year-of-artificial-intelligence"},{"type":3,"url":"medium://a-year-of-artificial-intelligence"}]},{"url":"https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5","alts":[{"type":3,"url":"medium://p/d702d65eb919"},{"type":2,"url":"medium://p/d702d65eb919"}]},{"url":"https://www.youtube.com/watch?v=q0pm3BrIUFo","alts":[{"type":3,"url":"vnd.youtube://www.youtube.com/watch?v=q0pm3BrIUFo&feature=applinks"},{"type":2,"url":"vnd.youtube://www.youtube.com/watch?v=q0pm3BrIUFo&feature=applinks"}]}],"version":"0.3","generatedAt":1471922593655},"isLockedPreviewOnly":false},"coverless":true,"slug":"rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","previewContent":{"bodyModel":{"paragraphs":[{"name":"4d27","type":4,"text":"","markups":[],"layout":10,"metadata":{"id":"1*pKv3DL-enonlNJxzqhlWkQ.jpeg","originalWidth":2750,"originalHeight":1833}},{"name":"ae68","type":3,"text":"Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained","markups":[],"alignment":1},{"name":"892d","type":13,"text":"Do you know the chain…","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","approvedHomeCollectionId":"bb87da25612c","approvedHomeCollection":{"id":"bb87da25612c","name":"A year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING"],"creatorId":"cb55958ea3bb","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","shortDescription":"2016: Our venture into the mathematics, science, and…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":197,"postCount":6,"activeAt":1465125336105},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A year of Artificial Intelligence","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A year of Artificial Intelligence."},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":3,"postIds":[]}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"tintColor":"#FF16A085","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF049B80","point":0},{"color":"#FF188F77","point":0.1},{"color":"#FF20846E","point":0.2},{"color":"#FF247865","point":0.3},{"color":"#FF266C5B","point":0.4},{"color":"#FF266052","point":0.5},{"color":"#FF245447","point":0.6},{"color":"#FF21473D","point":0.7},{"color":"#FF1C3A32","point":0.8},{"color":"#FF162C26","point":0.9},{"color":"#FF0E1D19","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF16A085","point":0},{"color":"#FF40AB92","point":0.1},{"color":"#FF5BB69F","point":0.2},{"color":"#FF72C1AB","point":0.3},{"color":"#FF87CBB7","point":0.4},{"color":"#FF9AD5C3","point":0.5},{"color":"#FFADDFCF","point":0.6},{"color":"#FFBEE9DB","point":0.7},{"color":"#FFD0F2E6","point":0.8},{"color":"#FFE1FBF1","point":0.9},{"color":"#FFF1FFFD","point":1}],"backgroundColor":"#FF16A085"},"highlightSpectrum":{"colorPoints":[{"color":"#FFE1F9F0","point":0},{"color":"#FFDCF9EE","point":0.1},{"color":"#FFD6F8EC","point":0.2},{"color":"#FFD0F7EA","point":0.3},{"color":"#FFC9F7E8","point":0.4},{"color":"#FFC3F6E6","point":0.5},{"color":"#FFBCF6E3","point":0.6},{"color":"#FFB5F5E1","point":0.7},{"color":"#FFAEF5DF","point":0.8},{"color":"#FFA7F4DD","point":0.9},{"color":"#FF9FF4DB","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"fullTextRssFeed":0,"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","mediumUrl":"https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"isSponsored":false,"isRequestToPubDisabled":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"type":"Post"},"collaborators":[],"membershipPlans":[],"footerPromo":{"promoId":"read_next_sign_in_tnt","isDismissable":false,"imageId":"1*trOZJzNWOEyh79Y1od6W-Q.png","streamIndex":0},"collectionUserRelations":[],"mode":null,"references":{"User":{"cb55958ea3bb":{"userId":"cb55958ea3bb","name":"Rohan Kapur","username":"mckapur","createdAt":1383813905817,"lastPostCreatedAt":1474026363557,"imageId":"1*f_drXOgflIo8yaxCbSDOmQ.jpeg","backgroundImageId":"","bio":"rohankapur.com","twitterScreenName":"MCKapur","socialStats":{"userId":"cb55958ea3bb","usersFollowedCount":189,"usersFollowedByCount":374,"type":"SocialStats"},"social":{"userId":"lo_91b04744946","targetUserId":"cb55958ea3bb","type":"Social"},"facebookAccountId":"1004843339565980","allowNotes":1,"type":"User"}},"Collection":{"bb87da25612c":{"id":"bb87da25612c","name":"A year of Artificial Intelligence","slug":"a-year-of-artificial-intelligence","tags":["DATA SCIENCE","ARTIFICIAL INTELLIGENCE","MACHINE LEARNING"],"creatorId":"cb55958ea3bb","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","shortDescription":"2016: Our venture into the mathematics, science, and…","image":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":197,"postCount":6,"activeAt":1465125336105},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":""},"logo":{"imageId":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","filter":"","backgroundSize":"","originalWidth":1500,"originalHeight":1000,"strategy":"resample","height":0,"width":0},"twitterUsername":"mckapur","facebookPageName":"mckapur","publicEmail":"me@rohankapur.com","domain":"ayearofai.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"A year of Artificial Intelligence","description":"2016: Our venture into the mathematics, science, and philosophy of Artificial Intelligence.","backgroundImage":{"id":"1*UlHnUWTtTHWOrOuJdHjVww.png","originalWidth":2000,"originalHeight":1333},"logoImage":{"id":"1*NZsNSuNxe_O2YW1ybboOvA.jpeg","originalWidth":1500,"originalHeight":1000,"alt":"A year of Artificial Intelligence."},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":3,"postIds":[]}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"tintColor":"#FF16A085","lightText":true,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF049B80","point":0},{"color":"#FF188F77","point":0.1},{"color":"#FF20846E","point":0.2},{"color":"#FF247865","point":0.3},{"color":"#FF266C5B","point":0.4},{"color":"#FF266052","point":0.5},{"color":"#FF245447","point":0.6},{"color":"#FF21473D","point":0.7},{"color":"#FF1C3A32","point":0.8},{"color":"#FF162C26","point":0.9},{"color":"#FF0E1D19","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF16A085","point":0},{"color":"#FF40AB92","point":0.1},{"color":"#FF5BB69F","point":0.2},{"color":"#FF72C1AB","point":0.3},{"color":"#FF87CBB7","point":0.4},{"color":"#FF9AD5C3","point":0.5},{"color":"#FFADDFCF","point":0.6},{"color":"#FFBEE9DB","point":0.7},{"color":"#FFD0F2E6","point":0.8},{"color":"#FFE1FBF1","point":0.9},{"color":"#FFF1FFFD","point":1}],"backgroundColor":"#FF16A085"},"highlightSpectrum":{"colorPoints":[{"color":"#FFE1F9F0","point":0},{"color":"#FFDCF9EE","point":0.1},{"color":"#FFD6F8EC","point":0.2},{"color":"#FFD0F7EA","point":0.3},{"color":"#FFC9F7E8","point":0.4},{"color":"#FFC3F6E6","point":0.5},{"color":"#FFBCF6E3","point":0.6},{"color":"#FFB5F5E1","point":0.7},{"color":"#FFAEF5DF","point":0.8},{"color":"#FFA7F4DD","point":0.9},{"color":"#FF9FF4DB","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[{"type":1,"title":"Today I Learned","tagSlug":"today-i-learned","url":"https://ayearofai.com/tagged/today-i-learned","source":"tagSlug"},{"type":1,"title":"Algorithms","tagSlug":"algorithms","url":"https://ayearofai.com/tagged/algorithms","source":"tagSlug"},{"type":1,"title":"Case Studies","tagSlug":"case-studies","url":"https://ayearofai.com/tagged/case-studies","source":"tagSlug"},{"type":1,"title":"Philosophical","tagSlug":"philosophical","url":"https://ayearofai.com/tagged/philosophical","source":"tagSlug"},{"type":1,"title":"Meta","tagSlug":"meta","url":"https://ayearofai.com/tagged/meta","source":"tagSlug"}],"fullTextRssFeed":0,"colorBehavior":2,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"type":"Collection"}},"Social":{"cb55958ea3bb":{"userId":"lo_91b04744946","targetUserId":"cb55958ea3bb","type":"Social"}},"SocialStats":{"cb55958ea3bb":{"userId":"cb55958ea3bb","usersFollowedCount":189,"usersFollowedByCount":374,"type":"SocialStats"}}}})
// ]]></script><div class="surface-scrollOverlay"></div><script src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/main-common-async.js" charset="UTF-8"></script><script src="Rohan%20&amp;%20Lenny%20%231:%20Neural%20Networks%20&amp;%20The%20Backpropagation%20Algorithm,%20Explained_files/main-notes.js" charset="UTF-8"></script></body></html>